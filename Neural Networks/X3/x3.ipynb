{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6dd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Imports (add tensorflow)\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf                                   # ‚Üê new\n",
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from scipy.stats import norm; warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d754859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Load\n",
    "df = pd.read_csv('option data variable.csv', parse_dates=['date','exdate'])\n",
    "df.dropna(inplace=True); df['strike_price'] /= 1_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0776ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Features (add log-moneyness)\n",
    "df['mid_price']   = (df['best_bid'] + df['best_offer'])/2\n",
    "df['days_to_exp'] = (df['exdate'] - df['date']).dt.days\n",
    "df['is_call']     = (df['cp_flag']=='C').astype(int)\n",
    "df['log_mny']     = np.log(df['underlying_price']/df['strike_price'])\n",
    "df['log_mny2']    = df['log_mny']**2\n",
    "\n",
    "X_COLS = ['underlying_price','strike_price','impl_volatility',\n",
    "          'risk_free_rate','days_to_exp','is_call','log_mny','log_mny2']\n",
    "Y_COLS = ['mid_price','delta','gamma','vega','theta']\n",
    "\n",
    "df = df.dropna(subset=X_COLS + Y_COLS).sort_values('date').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29728c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>secid</th>\n",
       "      <th>strike_price</th>\n",
       "      <th>best_bid</th>\n",
       "      <th>best_offer</th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>delta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>vega</th>\n",
       "      <th>theta</th>\n",
       "      <th>exdate</th>\n",
       "      <th>cp_flag</th>\n",
       "      <th>mid_price</th>\n",
       "      <th>risk_free_rate</th>\n",
       "      <th>underlying_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>101369.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>4.069927</td>\n",
       "      <td>0.974350</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.103827</td>\n",
       "      <td>-25.86654</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>C</td>\n",
       "      <td>9.25</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>19.0725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>101369.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.207709</td>\n",
       "      <td>0.986672</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.059599</td>\n",
       "      <td>-11.77059</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>C</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>19.0725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>101369.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2.983993</td>\n",
       "      <td>0.985723</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>0.063206</td>\n",
       "      <td>-11.62743</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>C</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>19.0725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>101369.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.769642</td>\n",
       "      <td>0.984681</td>\n",
       "      <td>0.008022</td>\n",
       "      <td>0.067096</td>\n",
       "      <td>-11.47521</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>C</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>19.0725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>101369.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.426838</td>\n",
       "      <td>0.863271</td>\n",
       "      <td>0.019603</td>\n",
       "      <td>0.380452</td>\n",
       "      <td>-148.89860</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>C</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>19.0725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     secid  strike_price  best_bid  best_offer  impl_volatility  \\\n",
       "0 2018-01-02  101369.0          10.0       7.0        11.5         4.069927   \n",
       "1 2018-01-02  101369.0          10.5       6.4        11.0         3.207709   \n",
       "2 2018-01-02  101369.0          11.0       5.9        10.5         2.983993   \n",
       "3 2018-01-02  101369.0          11.5       5.4        10.0         2.769642   \n",
       "4 2018-01-02  101369.0          12.0       6.8         9.6         6.426838   \n",
       "\n",
       "      delta     gamma      vega      theta     exdate cp_flag  mid_price  \\\n",
       "0  0.974350  0.008452  0.103827  -25.86654 2018-01-05       C       9.25   \n",
       "1  0.986672  0.006145  0.059599  -11.77059 2018-01-05       C       8.70   \n",
       "2  0.985723  0.007009  0.063206  -11.62743 2018-01-05       C       8.20   \n",
       "3  0.984681  0.008022  0.067096  -11.47521 2018-01-05       C       7.70   \n",
       "4  0.863271  0.019603  0.380452 -148.89860 2018-01-05       C       8.20   \n",
       "\n",
       "   risk_free_rate  underlying_price  \n",
       "0          0.0144           19.0725  \n",
       "1          0.0144           19.0725  \n",
       "2          0.0144           19.0725  \n",
       "3          0.0144           19.0725  \n",
       "4          0.0144           19.0725  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397d22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Chronological split (unchanged) ...\n",
    "df_call, df_put = df[df.is_call==1], df[df.is_call==0]\n",
    "split = lambda g: (g.iloc[:int(.98*len(g))],\n",
    "                   g.iloc[int(.98*len(g)):int(.985*len(g))],\n",
    "                   g.iloc[int(.985*len(g)):])\n",
    "\n",
    "call_tr, call_val, call_te = split(df_call)\n",
    "put_tr , put_val , put_te  = split(df_put)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5e8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Scaling (unchanged) ...\n",
    "x_scal = StandardScaler().fit(pd.concat([call_tr,put_tr])[X_COLS])\n",
    "ysc_c  = StandardScaler().fit(call_tr[Y_COLS])\n",
    "ysc_p  = StandardScaler().fit(put_tr[Y_COLS])\n",
    "\n",
    "prep = lambda g,xs,ys: (xs.transform(g[X_COLS]), ys.transform(g[Y_COLS]))\n",
    "cXtr,cYtr = prep(call_tr,x_scal,ysc_c); cXva,cYva = prep(call_val,x_scal,ysc_c); cXte,cYte = prep(call_te,x_scal,ysc_c)\n",
    "pXtr,pYtr = prep(put_tr ,x_scal,ysc_p); pXva,pYva = prep(put_val ,x_scal,ysc_p); pXte,pYte = prep(put_te ,x_scal,ysc_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcca0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) MLP factory (extra layer + Œ∏-weight in loss)\n",
    "def build_mlp(indim, hidden=512, layers=6, dropout=.3, theta_w=2.0):\n",
    "    x = Input(shape=(indim,)); h = Dense(hidden)(x); h=LeakyReLU()(h)\n",
    "    for _ in range(layers-1):\n",
    "        h=Dense(hidden)(h); h=BatchNormalization()(h); h=LeakyReLU()(h); h=Dropout(dropout)(h)\n",
    "    out = Dense(len(Y_COLS))(h)\n",
    "    w = tf.constant([1.,1.,1.,1.,theta_w], dtype='float32')\n",
    "    loss = lambda y_t,y_p: tf.reduce_mean(w * tf.square(y_t - y_p), axis=-1)\n",
    "    m = tf.keras.Model(x,out); m.compile('adam', loss=loss); return m\n",
    "\n",
    "CB = [EarlyStopping(patience=15,restore_best_weights=True),\n",
    "      ReduceLROnPlateau(factor=.5,patience=7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e5a9c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2121/2121 [==============================] - 33s 14ms/step - loss: 0.2157 - val_loss: 0.2731 - lr: 0.0010\n",
      "Epoch 2/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.1091 - val_loss: 0.1892 - lr: 0.0010\n",
      "Epoch 3/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0949 - val_loss: 0.1408 - lr: 0.0010\n",
      "Epoch 4/60\n",
      "2121/2121 [==============================] - 29s 13ms/step - loss: 0.0878 - val_loss: 0.1158 - lr: 0.0010\n",
      "Epoch 5/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0831 - val_loss: 0.0929 - lr: 0.0010\n",
      "Epoch 6/60\n",
      "2121/2121 [==============================] - 29s 13ms/step - loss: 0.0794 - val_loss: 0.1132 - lr: 0.0010\n",
      "Epoch 7/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0760 - val_loss: 0.0874 - lr: 0.0010\n",
      "Epoch 8/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0734 - val_loss: 0.0968 - lr: 0.0010\n",
      "Epoch 9/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0710 - val_loss: 0.1272 - lr: 0.0010\n",
      "Epoch 10/60\n",
      "2121/2121 [==============================] - 29s 13ms/step - loss: 0.0699 - val_loss: 0.0600 - lr: 0.0010\n",
      "Epoch 11/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0685 - val_loss: 0.0956 - lr: 0.0010\n",
      "Epoch 12/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0666 - val_loss: 0.2265 - lr: 0.0010\n",
      "Epoch 13/60\n",
      "2121/2121 [==============================] - 29s 13ms/step - loss: 0.0658 - val_loss: 0.0815 - lr: 0.0010\n",
      "Epoch 14/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0646 - val_loss: 0.0783 - lr: 0.0010\n",
      "Epoch 15/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0638 - val_loss: 0.0557 - lr: 0.0010\n",
      "Epoch 16/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0628 - val_loss: 0.1094 - lr: 0.0010\n",
      "Epoch 17/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0621 - val_loss: 0.1030 - lr: 0.0010\n",
      "Epoch 18/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0608 - val_loss: 0.0450 - lr: 0.0010\n",
      "Epoch 19/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0601 - val_loss: 0.2547 - lr: 0.0010\n",
      "Epoch 20/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0593 - val_loss: 0.1249 - lr: 0.0010\n",
      "Epoch 21/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0590 - val_loss: 0.4883 - lr: 0.0010\n",
      "Epoch 22/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0588 - val_loss: 0.0623 - lr: 0.0010\n",
      "Epoch 23/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0580 - val_loss: 0.1137 - lr: 0.0010\n",
      "Epoch 24/60\n",
      "2121/2121 [==============================] - 29s 13ms/step - loss: 0.0572 - val_loss: 0.0830 - lr: 0.0010\n",
      "Epoch 25/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0572 - val_loss: 0.0530 - lr: 0.0010\n",
      "Epoch 26/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0493 - val_loss: 0.1036 - lr: 5.0000e-04\n",
      "Epoch 27/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0492 - val_loss: 0.0280 - lr: 5.0000e-04\n",
      "Epoch 28/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0482 - val_loss: 0.0576 - lr: 5.0000e-04\n",
      "Epoch 29/60\n",
      "2121/2121 [==============================] - 29s 13ms/step - loss: 0.0484 - val_loss: 0.0751 - lr: 5.0000e-04\n",
      "Epoch 30/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0476 - val_loss: 0.0343 - lr: 5.0000e-04\n",
      "Epoch 31/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0472 - val_loss: 0.0618 - lr: 5.0000e-04\n",
      "Epoch 32/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0475 - val_loss: 0.0671 - lr: 5.0000e-04\n",
      "Epoch 33/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0467 - val_loss: 0.0353 - lr: 5.0000e-04\n",
      "Epoch 34/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0466 - val_loss: 0.0354 - lr: 5.0000e-04\n",
      "Epoch 35/60\n",
      "2121/2121 [==============================] - 29s 13ms/step - loss: 0.0427 - val_loss: 0.0666 - lr: 2.5000e-04\n",
      "Epoch 36/60\n",
      "2121/2121 [==============================] - 29s 13ms/step - loss: 0.0418 - val_loss: 0.0422 - lr: 2.5000e-04\n",
      "Epoch 37/60\n",
      "2121/2121 [==============================] - 29s 13ms/step - loss: 0.0417 - val_loss: 0.0256 - lr: 2.5000e-04\n",
      "Epoch 38/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0414 - val_loss: 0.0430 - lr: 2.5000e-04\n",
      "Epoch 39/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0414 - val_loss: 0.0957 - lr: 2.5000e-04\n",
      "Epoch 40/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0415 - val_loss: 0.0340 - lr: 2.5000e-04\n",
      "Epoch 41/60\n",
      "2121/2121 [==============================] - 29s 14ms/step - loss: 0.0411 - val_loss: 0.0561 - lr: 2.5000e-04\n",
      "Epoch 42/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0410 - val_loss: 0.0323 - lr: 2.5000e-04\n",
      "Epoch 43/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0411 - val_loss: 0.0510 - lr: 2.5000e-04\n",
      "Epoch 44/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0407 - val_loss: 0.0332 - lr: 2.5000e-04\n",
      "Epoch 45/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0383 - val_loss: 0.0350 - lr: 1.2500e-04\n",
      "Epoch 46/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0382 - val_loss: 0.0188 - lr: 1.2500e-04\n",
      "Epoch 47/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0380 - val_loss: 0.0333 - lr: 1.2500e-04\n",
      "Epoch 48/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0377 - val_loss: 0.0338 - lr: 1.2500e-04\n",
      "Epoch 49/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0378 - val_loss: 0.0281 - lr: 1.2500e-04\n",
      "Epoch 50/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0377 - val_loss: 0.0447 - lr: 1.2500e-04\n",
      "Epoch 51/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0378 - val_loss: 0.0371 - lr: 1.2500e-04\n",
      "Epoch 52/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0376 - val_loss: 0.0332 - lr: 1.2500e-04\n",
      "Epoch 53/60\n",
      "2121/2121 [==============================] - 27s 13ms/step - loss: 0.0374 - val_loss: 0.0402 - lr: 1.2500e-04\n",
      "Epoch 54/60\n",
      "2121/2121 [==============================] - 27s 13ms/step - loss: 0.0361 - val_loss: 0.0271 - lr: 6.2500e-05\n",
      "Epoch 55/60\n",
      "2121/2121 [==============================] - 27s 13ms/step - loss: 0.0359 - val_loss: 0.0396 - lr: 6.2500e-05\n",
      "Epoch 56/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0359 - val_loss: 0.0251 - lr: 6.2500e-05\n",
      "Epoch 57/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0357 - val_loss: 0.0236 - lr: 6.2500e-05\n",
      "Epoch 58/60\n",
      "2121/2121 [==============================] - 28s 13ms/step - loss: 0.0357 - val_loss: 0.0300 - lr: 6.2500e-05\n",
      "Epoch 59/60\n",
      "2121/2121 [==============================] - 27s 13ms/step - loss: 0.0358 - val_loss: 0.0357 - lr: 6.2500e-05\n",
      "Epoch 60/60\n",
      "2121/2121 [==============================] - 27s 13ms/step - loss: 0.0357 - val_loss: 0.0311 - lr: 6.2500e-05\n",
      "Epoch 1/60\n",
      "2014/2014 [==============================] - 29s 13ms/step - loss: 0.2244 - val_loss: 0.1419 - lr: 0.0010\n",
      "Epoch 2/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.1192 - val_loss: 0.0779 - lr: 0.0010\n",
      "Epoch 3/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.1047 - val_loss: 0.1294 - lr: 0.0010\n",
      "Epoch 4/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0985 - val_loss: 0.0947 - lr: 0.0010\n",
      "Epoch 5/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0930 - val_loss: 0.0738 - lr: 0.0010\n",
      "Epoch 6/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0884 - val_loss: 0.0679 - lr: 0.0010\n",
      "Epoch 7/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0867 - val_loss: 0.1228 - lr: 0.0010\n",
      "Epoch 8/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0833 - val_loss: 0.0885 - lr: 0.0010\n",
      "Epoch 9/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0825 - val_loss: 0.0631 - lr: 0.0010\n",
      "Epoch 10/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0800 - val_loss: 0.1012 - lr: 0.0010\n",
      "Epoch 11/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0777 - val_loss: 0.1252 - lr: 0.0010\n",
      "Epoch 12/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0769 - val_loss: 0.0498 - lr: 0.0010\n",
      "Epoch 13/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0755 - val_loss: 0.0640 - lr: 0.0010\n",
      "Epoch 14/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0738 - val_loss: 0.0771 - lr: 0.0010\n",
      "Epoch 15/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0728 - val_loss: 0.0690 - lr: 0.0010\n",
      "Epoch 16/60\n",
      "2014/2014 [==============================] - 27s 13ms/step - loss: 0.0716 - val_loss: 0.0646 - lr: 0.0010\n",
      "Epoch 17/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0711 - val_loss: 0.0578 - lr: 0.0010\n",
      "Epoch 18/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0707 - val_loss: 0.0335 - lr: 0.0010\n",
      "Epoch 19/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0699 - val_loss: 0.0385 - lr: 0.0010\n",
      "Epoch 20/60\n",
      "2014/2014 [==============================] - 31s 15ms/step - loss: 0.0691 - val_loss: 0.0888 - lr: 0.0010\n",
      "Epoch 21/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0679 - val_loss: 0.0459 - lr: 0.0010\n",
      "Epoch 22/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0671 - val_loss: 0.0696 - lr: 0.0010\n",
      "Epoch 23/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0667 - val_loss: 0.0799 - lr: 0.0010\n",
      "Epoch 24/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0674 - val_loss: 0.0434 - lr: 0.0010\n",
      "Epoch 25/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0655 - val_loss: 0.0483 - lr: 0.0010\n",
      "Epoch 26/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0581 - val_loss: 0.0368 - lr: 5.0000e-04\n",
      "Epoch 27/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0574 - val_loss: 0.0324 - lr: 5.0000e-04\n",
      "Epoch 28/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0561 - val_loss: 0.0434 - lr: 5.0000e-04\n",
      "Epoch 29/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0563 - val_loss: 0.3345 - lr: 5.0000e-04\n",
      "Epoch 30/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0565 - val_loss: 0.0757 - lr: 5.0000e-04\n",
      "Epoch 31/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0558 - val_loss: 0.0363 - lr: 5.0000e-04\n",
      "Epoch 32/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0561 - val_loss: 0.0427 - lr: 5.0000e-04\n",
      "Epoch 33/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0554 - val_loss: 0.0452 - lr: 5.0000e-04\n",
      "Epoch 34/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0548 - val_loss: 0.0337 - lr: 5.0000e-04\n",
      "Epoch 35/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0508 - val_loss: 0.0265 - lr: 2.5000e-04\n",
      "Epoch 36/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0499 - val_loss: 0.0383 - lr: 2.5000e-04\n",
      "Epoch 37/60\n",
      "2014/2014 [==============================] - 27s 13ms/step - loss: 0.0497 - val_loss: 0.0256 - lr: 2.5000e-04\n",
      "Epoch 38/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0497 - val_loss: 0.0331 - lr: 2.5000e-04\n",
      "Epoch 39/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0489 - val_loss: 0.0279 - lr: 2.5000e-04\n",
      "Epoch 40/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0492 - val_loss: 0.0255 - lr: 2.5000e-04\n",
      "Epoch 41/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0488 - val_loss: 0.0343 - lr: 2.5000e-04\n",
      "Epoch 42/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0489 - val_loss: 0.0373 - lr: 2.5000e-04\n",
      "Epoch 43/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0487 - val_loss: 0.0263 - lr: 2.5000e-04\n",
      "Epoch 44/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0482 - val_loss: 0.0249 - lr: 2.5000e-04\n",
      "Epoch 45/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0485 - val_loss: 0.0262 - lr: 2.5000e-04\n",
      "Epoch 46/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0479 - val_loss: 0.0254 - lr: 2.5000e-04\n",
      "Epoch 47/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0476 - val_loss: 0.0230 - lr: 2.5000e-04\n",
      "Epoch 48/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0482 - val_loss: 0.0368 - lr: 2.5000e-04\n",
      "Epoch 49/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0479 - val_loss: 0.0449 - lr: 2.5000e-04\n",
      "Epoch 50/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0472 - val_loss: 0.0247 - lr: 2.5000e-04\n",
      "Epoch 51/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0474 - val_loss: 0.0244 - lr: 2.5000e-04\n",
      "Epoch 52/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0474 - val_loss: 0.0406 - lr: 2.5000e-04\n",
      "Epoch 53/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0477 - val_loss: 0.0268 - lr: 2.5000e-04\n",
      "Epoch 54/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0473 - val_loss: 0.0316 - lr: 2.5000e-04\n",
      "Epoch 55/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0450 - val_loss: 0.0210 - lr: 1.2500e-04\n",
      "Epoch 56/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0447 - val_loss: 0.0302 - lr: 1.2500e-04\n",
      "Epoch 57/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0442 - val_loss: 0.0230 - lr: 1.2500e-04\n",
      "Epoch 58/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0441 - val_loss: 0.0245 - lr: 1.2500e-04\n",
      "Epoch 59/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0440 - val_loss: 0.0242 - lr: 1.2500e-04\n",
      "Epoch 60/60\n",
      "2014/2014 [==============================] - 26s 13ms/step - loss: 0.0438 - val_loss: 0.0208 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14040c23880>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 6) Train\n",
    "call_m = build_mlp(cXtr.shape[1]); call_m.fit(cXtr,cYtr,validation_data=(cXva,cYva),\n",
    "                                             epochs=60,batch_size=4096,callbacks=CB,verbose=1)\n",
    "put_m  = build_mlp(pXtr.shape[1]); put_m.fit(pXtr,pYtr,validation_data=(pXva,pYva),\n",
    "                                             epochs=60,batch_size=4096,callbacks=CB,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cee0fbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4154/4154 [==============================] - 14s 3ms/step\n",
      "3946/3946 [==============================] - 13s 3ms/step\n",
      "\n",
      "CALL MODEL\n",
      "mid_price   MSE=3.545308  MAE=1.121932  R¬≤=0.9988\n",
      "delta       MSE=0.000647  MAE=0.015751  R¬≤=0.9955\n",
      "gamma       MSE=0.000474  MAE=0.004118  R¬≤=0.9317\n",
      "vega        MSE=5.551839  MAE=1.349863  R¬≤=0.9971\n",
      "theta       MSE=59.677525  MAE=2.217834  R¬≤=0.9601\n",
      "\n",
      "PUT MODEL\n",
      "mid_price   MSE=1.197582  MAE=0.624755  R¬≤=0.9986\n",
      "delta       MSE=0.000831  MAE=0.017550  R¬≤=0.9935\n",
      "gamma       MSE=0.000338  MAE=0.004354  R¬≤=0.9596\n",
      "vega        MSE=11.092182  MAE=1.652020  R¬≤=0.9931\n",
      "theta       MSE=31.069621  MAE=2.073442  R¬≤=0.9516\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7) Evaluate\n",
    "c_pred = ysc_c.inverse_transform(call_m.predict(cXte)); c_true = ysc_c.inverse_transform(cYte)\n",
    "p_pred = ysc_p.inverse_transform( put_m.predict(pXte)); p_true = ysc_p.inverse_transform(pYte)\n",
    "\n",
    "for tag,t,p in [('CALL',c_true,c_pred),('PUT',p_true,p_pred)]:\n",
    "    print(f'\\n{tag} MODEL'); \n",
    "    for i,g in enumerate(Y_COLS):\n",
    "        print(f'{g:10s}  MSE={mean_squared_error(t[:,i],p[:,i]):.6f}  '\n",
    "              f'MAE={mean_absolute_error(t[:,i],p[:,i]):.6f}  '\n",
    "              f'R¬≤={r2_score(t[:,i],p[:,i]):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277164a",
   "metadata": {},
   "source": [
    "# OPTUNA Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 20:25:51,065] A new study created in memory with name: no-name-c51311f6-44f2-4c8b-a652-fb7e8f8a0664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢  Starting Optuna search for **CALL** model ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 20:44:21,178] Trial 0 finished with value: 0.03982483968138695 and parameters: {'units': 384, 'layers': 6, 'drop': 0.4369625828503292, 'theta_w': 3.0869315456096977, 'lr0': 0.0007867875770411421}. Best is trial 0 with value: 0.03982483968138695.\n",
      "[I 2025-04-28 21:05:42,584] Trial 1 finished with value: 0.032729409635066986 and parameters: {'units': 384, 'layers': 7, 'drop': 0.3377937041671903, 'theta_w': 3.014008012461961, 'lr0': 0.002278759197077704}. Best is trial 1 with value: 0.032729409635066986.\n",
      "[I 2025-04-28 21:28:40,493] Trial 2 finished with value: 0.043362416326999664 and parameters: {'units': 256, 'layers': 8, 'drop': 0.38863759237116585, 'theta_w': 3.0106973509153745, 'lr0': 0.0026463501601638136}. Best is trial 1 with value: 0.032729409635066986.\n",
      "[W 2025-04-28 21:28:51,167] Trial 3 failed with parameters: {'units': 768, 'layers': 8, 'drop': 0.2591726457929341, 'theta_w': 3.3904189195777947, 'lr0': 0.0013924993275376465} because of the following error: InternalError().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\jacks\\AppData\\Local\\Temp\\ipykernel_46188\\465551510.py\", line 74, in objective\n",
      "    model.fit(X_tr, Y_tr,\n",
      "  File \"c:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"c:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 102, in convert_to_eager_tensor\n",
      "    return ops.EagerTensor(value, ctx.device_name, dtype)\n",
      "tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n",
      "[W 2025-04-28 21:28:51,170] Trial 3 failed with value None.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 120\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# 4)  Run for CALLs and PUTs\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[0;32m    118\u001b[0m N_TRIALS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m          \u001b[38;5;66;03m# bump to 50-100 for a deeper search if you wish\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m call_model \u001b[38;5;241m=\u001b[39m \u001b[43moptuna_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcYtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcXva\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcYva\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCALL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m put_model  \u001b[38;5;241m=\u001b[39m optuna_search(pXtr, pYtr, pXva, pYva, tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m'\u001b[39m,  n_trials\u001b[38;5;241m=\u001b[39mN_TRIALS)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müéâ  Both optimised models saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_call.keras\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_put.keras\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 92\u001b[0m, in \u001b[0;36moptuna_search\u001b[1;34m(Xtr, Ytr, Xva, Yva, tag, n_trials)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müü¢  Starting Optuna search for **\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m** model ‚Ä¶\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXva\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYva\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m best params ‚Üí\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# build + train final model with full train+val data using best params\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[13], line 74\u001b[0m, in \u001b[0;36mmake_objective.<locals>.objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     68\u001b[0m cbs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     69\u001b[0m     EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     70\u001b[0m     LearningRateScheduler(\u001b[38;5;28;01mlambda\u001b[39;00m e: cos_sched(e), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     71\u001b[0m ]\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# ---------- edit #2: EPOCHS (capital) ----------\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, Y_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     80\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n",
      "File \u001b[1;32mc:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Optuna hyper-parameter sweep  (run after you have cXtr, cYtr, ‚Ä¶ ready)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import optuna, tensorflow as tf\n",
    "from keras.layers import (Input, Dense, LeakyReLU, BatchNormalization,\n",
    "                          Dropout, Concatenate)\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np \n",
    "\n",
    "############################################################################\n",
    "# 1)  Model-builder factory\n",
    "############################################################################\n",
    "def build_net(indim, hp):\n",
    "    \"\"\"Return compiled Keras model built from an Optuna trial dict `hp`.\"\"\"\n",
    "    units   = hp['units']\n",
    "    layers  = hp['layers']\n",
    "    dropout = hp['drop']\n",
    "    theta_w = hp['theta_w']\n",
    "    lr0     = hp['lr0']\n",
    "\n",
    "    x = Input(shape=(indim,))\n",
    "    h = Dense(units)(x); h = LeakyReLU()(h)\n",
    "\n",
    "    # residual block every 2 layers\n",
    "    for i in range(layers-1):\n",
    "        h_in = h\n",
    "        h = Dense(units)(h); h = BatchNormalization()(h); h = LeakyReLU()(h)\n",
    "        h = Dropout(dropout)(h)\n",
    "        if i % 2:                                    # add residual every 2nd layer\n",
    "            h = Concatenate()([h, h_in])\n",
    "\n",
    "    shared  = Dense(3)(h)        # mid_price, Œî, ŒΩ\n",
    "    gamma_h = Dense(1)(h)        # Œì\n",
    "    theta_h = Dense(1)(h)        # Œ∏\n",
    "    out = Concatenate()([shared, gamma_h, theta_h])\n",
    "\n",
    "    w = tf.constant([1., 1., 1., 3., theta_w], dtype='float32')\n",
    "    loss = lambda y_t, y_p: tf.reduce_mean(w * tf.square(y_t - y_p), axis=-1)\n",
    "\n",
    "    m = Model(x, out)\n",
    "    m.compile(tf.keras.optimizers.Adam(lr0), loss=loss)\n",
    "    return m\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 2)  Objective wrapper\n",
    "############################################################################\n",
    "def make_objective(X_tr, Y_tr, X_val, Y_val):\n",
    "    def objective(trial):\n",
    "        hp = {\n",
    "            'units'  : trial.suggest_int('units', 256, 768, step=128),\n",
    "            'layers' : trial.suggest_int('layers', 4, 8),\n",
    "            'drop'   : trial.suggest_float('drop', 0.15, 0.45),\n",
    "            'theta_w': trial.suggest_float('theta_w', 1.5, 3.5),\n",
    "            'lr0'    : trial.suggest_loguniform('lr0', 5e-4, 3e-3)\n",
    "        }\n",
    "\n",
    "        model  = build_net(X_tr.shape[1], hp)\n",
    "\n",
    "        EPOCHS = 60\n",
    "        # ---------- edit #1: use hp['lr0'] here ----------\n",
    "        cos_sched = CosineDecayRestarts(\n",
    "            initial_learning_rate=hp['lr0'],\n",
    "            first_decay_steps=EPOCHS // 2,\n",
    "            alpha=1e-5)\n",
    "\n",
    "        cbs = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            LearningRateScheduler(lambda e: cos_sched(e), verbose=0)\n",
    "        ]\n",
    "\n",
    "        # ---------- edit #2: EPOCHS (capital) ----------\n",
    "        model.fit(X_tr, Y_tr,\n",
    "                  validation_data=(X_val, Y_val),\n",
    "                  batch_size=4096, epochs=EPOCHS,\n",
    "                  callbacks=cbs, verbose=0)\n",
    "\n",
    "        val_loss = model.evaluate(X_val, Y_val, verbose=0)\n",
    "        tf.keras.backend.clear_session()\n",
    "        return val_loss\n",
    "    return objective\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 3)  Convenience runner\n",
    "############################################################################\n",
    "def optuna_search(Xtr, Ytr, Xva, Yva, tag, n_trials=30):\n",
    "    print(f\"\\nüü¢  Starting Optuna search for **{tag}** model ‚Ä¶\")\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(make_objective(Xtr, Ytr, Xva, Yva), n_trials=n_trials)\n",
    "    print(f\"‚úÖ  {tag} best params ‚Üí\", study.best_params)\n",
    "\n",
    "    # build + train final model with full train+val data using best params\n",
    "    best_hp = study.best_params\n",
    "    model   = build_net(Xtr.shape[1], best_hp)\n",
    "\n",
    "    X_full  = np.vstack([Xtr, Xva])\n",
    "    Y_full  = np.vstack([Ytr, Yva])\n",
    "\n",
    "    epochs = 60\n",
    "    cbs = [\n",
    "        EarlyStopping(patience=12, restore_best_weights=True),\n",
    "        CosineDecayRestarts(initial_learning_rate=best_hp['lr0'],\n",
    "                            first_decay_steps=epochs//2,\n",
    "                            alpha=1e-5)\n",
    "    ]\n",
    "    model.fit(X_full, Y_full, batch_size=4096, epochs=epochs,\n",
    "              callbacks=cbs, verbose=1)\n",
    "    model.save(f\"best_{tag.lower()}.keras\")\n",
    "    return model\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 4)  Run for CALLs and PUTs\n",
    "############################################################################\n",
    "N_TRIALS = 30          # bump to 50-100 for a deeper search if you wish\n",
    "\n",
    "call_model = optuna_search(cXtr, cYtr, cXva, cYva, tag='CALL', n_trials=N_TRIALS)\n",
    "put_model  = optuna_search(pXtr, pYtr, pXva, pYva, tag='PUT',  n_trials=N_TRIALS)\n",
    "\n",
    "print(\"\\nüéâ  Both optimised models saved to 'best_call.keras' and 'best_put.keras'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
