{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c9c77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization, Dropout, Concatenate\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f5c32d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0) Load and preprocess raw data\n",
    "# ----------------------------------------------------------------------------\n",
    "df = pd.read_csv('option data variable.csv', parse_dates=['date','exdate'])\n",
    "df.rename(columns={'exdate':'maturity'}, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df['strike_price'] /= 1_000\n",
    "\n",
    "# core features\n",
    "df['mid_price']   = (df['best_bid'] + df['best_offer']) / 2\n",
    "df['days_to_exp'] = (df['maturity'] - df['date']).dt.days\n",
    "df['is_call']     = (df['cp_flag']=='C').astype(int)\n",
    "df['log_mny']     = np.log(df['underlying_price']/df['strike_price'])\n",
    "df['log_mny2']    = df['log_mny']**2\n",
    "\n",
    "# helper for SSR labels\n",
    "df['F'] = df['underlying_price'] * np.exp(df['risk_free_rate'] * df['days_to_exp'] / 252)\n",
    "df['k'] = np.log(df['strike_price'] / df['F'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bcc87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1) Compute SSR labels and enrich features\n",
    "# ----------------------------------------------------------------------------\n",
    "def get_atm(group):\n",
    "    idx = (group['strike_price'] - group['F']).abs().idxmin()\n",
    "    return pd.Series({\n",
    "        'date': group.at[idx,'date'],\n",
    "        'maturity': group.at[idx,'maturity'],\n",
    "        'underlying_price': group.at[idx,'underlying_price'],\n",
    "        'atm_iv': group.at[idx,'impl_volatility']\n",
    "    })\n",
    "atm = df.groupby(['date','maturity']).apply(get_atm).reset_index(drop=True)\n",
    "atm = atm.sort_values(['maturity','date'])\n",
    "atm['dlnS'] = np.log(atm['underlying_price']).groupby(atm['maturity']).diff()\n",
    "atm['dIV']  = atm['atm_iv'].groupby(atm['maturity']).diff()\n",
    "atm['num']  = atm['dIV']/atm['dlnS']\n",
    "\n",
    "def skew_slope(group):\n",
    "    sub = group[np.abs(group['k'])<0.05]\n",
    "    if len(sub)<5: return np.nan\n",
    "    return np.polyfit(sub['k'], sub['impl_volatility'], 1)[0]\n",
    "skew = df.groupby(['date','maturity']).apply(skew_slope).reset_index(name='skew')\n",
    "\n",
    "sr = pd.merge(atm, skew, on=['date','maturity'], how='inner')\n",
    "sr['SSR_raw'] = sr['num']/sr['skew']\n",
    "sr['SSR'] = sr['SSR_raw'].clip(-5,5)\n",
    "# enrich df with SSR label and key vol features\n",
    "df = pd.merge(df, sr[['date','maturity','SSR','atm_iv','skew']], on=['date','maturity'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1836e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2) Feature & target setup\n",
    "# ----------------------------------------------------------------------------\n",
    "X_COLS = [\n",
    "    'underlying_price','strike_price','impl_volatility',\n",
    "    'risk_free_rate','days_to_exp','is_call','log_mny','log_mny2',\n",
    "    'atm_iv','skew'\n",
    "]\n",
    "GREEKS = ['mid_price','delta','gamma','vega','theta']\n",
    "Y_COLS = GREEKS + ['SSR']\n",
    "\n",
    "df.dropna(subset=X_COLS+Y_COLS, inplace=True)\n",
    "df_call, df_put = df[df['is_call']==1], df[df['is_call']==0]\n",
    "split=lambda g: (g.iloc[:int(.98*len(g))], g.iloc[int(.98*len(g)):int(.985*len(g))], g.iloc[int(.985*len(g)):])\n",
    "call_tr, call_va, call_te = split(df_call)\n",
    "put_tr, put_va, put_te    = split(df_put)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a7f9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3) Separate scaling for greeks & SSR & features\n",
    "# ----------------------------------------------------------------------------\n",
    "greek_scaler = StandardScaler().fit(pd.concat([call_tr[GREEKS], put_tr[GREEKS]]))\n",
    "ssr_scaler   = StandardScaler().fit(pd.concat([call_tr[['SSR']],  put_tr[['SSR']]]))\n",
    "feature_scaler = StandardScaler().fit(pd.concat([call_tr[X_COLS], put_tr[X_COLS]]))\n",
    "\n",
    "def prep(df_, xs, gs, ss):\n",
    "    X  = xs.transform(df_[X_COLS])\n",
    "    Yg = gs.transform(df_[GREEKS])\n",
    "    Ys = ss.transform(df_[['SSR']])\n",
    "    return X, np.hstack([Yg, Ys])\n",
    "\n",
    "cXtr, cYtr = prep(call_tr,  feature_scaler, greek_scaler, ssr_scaler)\n",
    "cXva, cYva = prep(call_va,  feature_scaler, greek_scaler, ssr_scaler)\n",
    "cXte, cYte = prep(call_te,  feature_scaler, greek_scaler, ssr_scaler)\n",
    "pXtr, pYtr = prep(put_tr,   feature_scaler, greek_scaler, ssr_scaler)\n",
    "pXva, pYva = prep(put_va,   feature_scaler, greek_scaler, ssr_scaler)\n",
    "pXte, pYte = prep(put_te,   feature_scaler, greek_scaler, ssr_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49b34f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4) Multi-task MLP: separate greeks & SSR heads\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_mlp_mt(indim, hidden=512, layers=6, dropout=0.3, theta_w=2.0, ssr_w=10.0):\n",
    "    x = Input(shape=(indim,))\n",
    "    h = Dense(hidden)(x); h = LeakyReLU()(h)\n",
    "    for _ in range(layers-1):\n",
    "        h = Dense(hidden)(h); h = BatchNormalization()(h)\n",
    "        h = LeakyReLU()(h); h = Dropout(dropout)(h)\n",
    "    out_g = Dense(len(GREEKS), name='greeks')(h)\n",
    "    out_s = Dense(1, name='ssr')(h)\n",
    "    model = Model(x, [out_g, out_s])\n",
    "    model.compile('adam', loss={'greeks':'mse','ssr':'mse'}, loss_weights={'greeks':1.0,'ssr':ssr_w})\n",
    "    return model\n",
    "\n",
    "CB = [EarlyStopping(patience=15, restore_best_weights=True), ReduceLROnPlateau(factor=0.5, patience=7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5d7d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2107/2107 [==============================] - 36s 16ms/step - loss: 10.4266 - greeks_loss: 0.3448 - ssr_loss: 1.0082 - val_loss: 11.5477 - val_greeks_loss: 0.1369 - val_ssr_loss: 1.1411 - lr: 0.0010\n",
      "Epoch 2/60\n",
      "2107/2107 [==============================] - 36s 17ms/step - loss: 9.6384 - greeks_loss: 0.1691 - ssr_loss: 0.9469 - val_loss: 11.1828 - val_greeks_loss: 0.0834 - val_ssr_loss: 1.1099 - lr: 0.0010\n",
      "Epoch 3/60\n",
      "2107/2107 [==============================] - 36s 17ms/step - loss: 9.4744 - greeks_loss: 0.1341 - ssr_loss: 0.9340 - val_loss: 11.0091 - val_greeks_loss: 0.0669 - val_ssr_loss: 1.0942 - lr: 0.0010\n",
      "Epoch 4/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 9.3667 - greeks_loss: 0.1203 - ssr_loss: 0.9246 - val_loss: 10.9483 - val_greeks_loss: 0.0528 - val_ssr_loss: 1.0895 - lr: 0.0010\n",
      "Epoch 5/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 9.2817 - greeks_loss: 0.1122 - ssr_loss: 0.9169 - val_loss: 10.7226 - val_greeks_loss: 0.0569 - val_ssr_loss: 1.0666 - lr: 0.0010\n",
      "Epoch 6/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 9.2077 - greeks_loss: 0.1087 - ssr_loss: 0.9099 - val_loss: 10.6726 - val_greeks_loss: 0.0554 - val_ssr_loss: 1.0617 - lr: 0.0010\n",
      "Epoch 7/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 9.1324 - greeks_loss: 0.1039 - ssr_loss: 0.9028 - val_loss: 10.5773 - val_greeks_loss: 0.0436 - val_ssr_loss: 1.0534 - lr: 0.0010\n",
      "Epoch 8/60\n",
      "2107/2107 [==============================] - 35s 16ms/step - loss: 9.0632 - greeks_loss: 0.1012 - ssr_loss: 0.8962 - val_loss: 10.2973 - val_greeks_loss: 0.0561 - val_ssr_loss: 1.0241 - lr: 0.0010\n",
      "Epoch 9/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.9940 - greeks_loss: 0.1013 - ssr_loss: 0.8893 - val_loss: 10.3719 - val_greeks_loss: 0.0602 - val_ssr_loss: 1.0312 - lr: 0.0010\n",
      "Epoch 10/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.9246 - greeks_loss: 0.1006 - ssr_loss: 0.8824 - val_loss: 10.2769 - val_greeks_loss: 0.0442 - val_ssr_loss: 1.0233 - lr: 0.0010\n",
      "Epoch 11/60\n",
      "2107/2107 [==============================] - 35s 17ms/step - loss: 8.8593 - greeks_loss: 0.1010 - ssr_loss: 0.8758 - val_loss: 9.8848 - val_greeks_loss: 0.0459 - val_ssr_loss: 0.9839 - lr: 0.0010\n",
      "Epoch 12/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.7900 - greeks_loss: 0.1014 - ssr_loss: 0.8689 - val_loss: 9.8635 - val_greeks_loss: 0.0521 - val_ssr_loss: 0.9811 - lr: 0.0010\n",
      "Epoch 13/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.7238 - greeks_loss: 0.1029 - ssr_loss: 0.8621 - val_loss: 9.5958 - val_greeks_loss: 0.0487 - val_ssr_loss: 0.9547 - lr: 0.0010\n",
      "Epoch 14/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.6645 - greeks_loss: 0.1021 - ssr_loss: 0.8562 - val_loss: 9.5481 - val_greeks_loss: 0.0529 - val_ssr_loss: 0.9495 - lr: 0.0010\n",
      "Epoch 15/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.6076 - greeks_loss: 0.1026 - ssr_loss: 0.8505 - val_loss: 9.3331 - val_greeks_loss: 0.0477 - val_ssr_loss: 0.9285 - lr: 0.0010\n",
      "Epoch 16/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.5606 - greeks_loss: 0.1038 - ssr_loss: 0.8457 - val_loss: 9.3674 - val_greeks_loss: 0.0616 - val_ssr_loss: 0.9306 - lr: 0.0010\n",
      "Epoch 17/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.5083 - greeks_loss: 0.1032 - ssr_loss: 0.8405 - val_loss: 9.1172 - val_greeks_loss: 0.0566 - val_ssr_loss: 0.9061 - lr: 0.0010\n",
      "Epoch 18/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.4641 - greeks_loss: 0.1039 - ssr_loss: 0.8360 - val_loss: 9.0555 - val_greeks_loss: 0.0642 - val_ssr_loss: 0.8991 - lr: 0.0010\n",
      "Epoch 19/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.4250 - greeks_loss: 0.1049 - ssr_loss: 0.8320 - val_loss: 9.2400 - val_greeks_loss: 0.0583 - val_ssr_loss: 0.9182 - lr: 0.0010\n",
      "Epoch 20/60\n",
      "2107/2107 [==============================] - 35s 17ms/step - loss: 8.3809 - greeks_loss: 0.1048 - ssr_loss: 0.8276 - val_loss: 9.1861 - val_greeks_loss: 0.0441 - val_ssr_loss: 0.9142 - lr: 0.0010\n",
      "Epoch 21/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.3420 - greeks_loss: 0.1058 - ssr_loss: 0.8236 - val_loss: 8.8697 - val_greeks_loss: 0.0590 - val_ssr_loss: 0.8811 - lr: 0.0010\n",
      "Epoch 22/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.3052 - greeks_loss: 0.1053 - ssr_loss: 0.8200 - val_loss: 8.7033 - val_greeks_loss: 0.0527 - val_ssr_loss: 0.8651 - lr: 0.0010\n",
      "Epoch 23/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.2666 - greeks_loss: 0.1050 - ssr_loss: 0.8162 - val_loss: 8.5612 - val_greeks_loss: 0.0540 - val_ssr_loss: 0.8507 - lr: 0.0010\n",
      "Epoch 24/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.2302 - greeks_loss: 0.1055 - ssr_loss: 0.8125 - val_loss: 8.4428 - val_greeks_loss: 0.0777 - val_ssr_loss: 0.8365 - lr: 0.0010\n",
      "Epoch 25/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.1942 - greeks_loss: 0.1056 - ssr_loss: 0.8089 - val_loss: 8.5122 - val_greeks_loss: 0.0707 - val_ssr_loss: 0.8442 - lr: 0.0010\n",
      "Epoch 26/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.1621 - greeks_loss: 0.1053 - ssr_loss: 0.8057 - val_loss: 8.2662 - val_greeks_loss: 0.0595 - val_ssr_loss: 0.8207 - lr: 0.0010\n",
      "Epoch 27/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.1280 - greeks_loss: 0.1066 - ssr_loss: 0.8021 - val_loss: 8.1256 - val_greeks_loss: 0.0597 - val_ssr_loss: 0.8066 - lr: 0.0010\n",
      "Epoch 28/60\n",
      "2107/2107 [==============================] - 33s 16ms/step - loss: 8.0901 - greeks_loss: 0.1067 - ssr_loss: 0.7983 - val_loss: 8.3667 - val_greeks_loss: 0.0528 - val_ssr_loss: 0.8314 - lr: 0.0010\n",
      "Epoch 29/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 8.0615 - greeks_loss: 0.1057 - ssr_loss: 0.7956 - val_loss: 7.9334 - val_greeks_loss: 0.0543 - val_ssr_loss: 0.7879 - lr: 0.0010\n",
      "Epoch 30/60\n",
      "2107/2107 [==============================] - 33s 16ms/step - loss: 8.0325 - greeks_loss: 0.1060 - ssr_loss: 0.7926 - val_loss: 8.0055 - val_greeks_loss: 0.0565 - val_ssr_loss: 0.7949 - lr: 0.0010\n",
      "Epoch 31/60\n",
      "2107/2107 [==============================] - 33s 16ms/step - loss: 8.0003 - greeks_loss: 0.1065 - ssr_loss: 0.7894 - val_loss: 7.8928 - val_greeks_loss: 0.0541 - val_ssr_loss: 0.7839 - lr: 0.0010\n",
      "Epoch 32/60\n",
      "2107/2107 [==============================] - 33s 16ms/step - loss: 7.9703 - greeks_loss: 0.1066 - ssr_loss: 0.7864 - val_loss: 7.7040 - val_greeks_loss: 0.0591 - val_ssr_loss: 0.7645 - lr: 0.0010\n",
      "Epoch 33/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 7.9418 - greeks_loss: 0.1069 - ssr_loss: 0.7835 - val_loss: 7.6194 - val_greeks_loss: 0.0474 - val_ssr_loss: 0.7572 - lr: 0.0010\n",
      "Epoch 34/60\n",
      "2107/2107 [==============================] - 33s 16ms/step - loss: 7.9152 - greeks_loss: 0.1061 - ssr_loss: 0.7809 - val_loss: 7.7317 - val_greeks_loss: 0.0632 - val_ssr_loss: 0.7669 - lr: 0.0010\n",
      "Epoch 35/60\n",
      "2107/2107 [==============================] - 33s 16ms/step - loss: 7.8844 - greeks_loss: 0.1071 - ssr_loss: 0.7777 - val_loss: 7.4310 - val_greeks_loss: 0.0557 - val_ssr_loss: 0.7375 - lr: 0.0010\n",
      "Epoch 36/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 7.8598 - greeks_loss: 0.1073 - ssr_loss: 0.7753 - val_loss: 7.4899 - val_greeks_loss: 0.0573 - val_ssr_loss: 0.7433 - lr: 0.0010\n",
      "Epoch 37/60\n",
      "2107/2107 [==============================] - 33s 16ms/step - loss: 7.8379 - greeks_loss: 0.1075 - ssr_loss: 0.7730 - val_loss: 7.2373 - val_greeks_loss: 0.0497 - val_ssr_loss: 0.7188 - lr: 0.0010\n",
      "Epoch 38/60\n",
      "2107/2107 [==============================] - 35s 17ms/step - loss: 7.8103 - greeks_loss: 0.1076 - ssr_loss: 0.7703 - val_loss: 7.2011 - val_greeks_loss: 0.0673 - val_ssr_loss: 0.7134 - lr: 0.0010\n",
      "Epoch 39/60\n",
      "2107/2107 [==============================] - 37s 18ms/step - loss: 7.7867 - greeks_loss: 0.1092 - ssr_loss: 0.7678 - val_loss: 7.2615 - val_greeks_loss: 0.0646 - val_ssr_loss: 0.7197 - lr: 0.0010\n",
      "Epoch 40/60\n",
      "2107/2107 [==============================] - 39s 18ms/step - loss: 7.7632 - greeks_loss: 0.1073 - ssr_loss: 0.7656 - val_loss: 7.2663 - val_greeks_loss: 0.0489 - val_ssr_loss: 0.7217 - lr: 0.0010\n",
      "Epoch 41/60\n",
      "2107/2107 [==============================] - 39s 18ms/step - loss: 7.7406 - greeks_loss: 0.1071 - ssr_loss: 0.7634 - val_loss: 7.2150 - val_greeks_loss: 0.0521 - val_ssr_loss: 0.7163 - lr: 0.0010\n",
      "Epoch 42/60\n",
      "2107/2107 [==============================] - 39s 19ms/step - loss: 7.7166 - greeks_loss: 0.1077 - ssr_loss: 0.7609 - val_loss: 6.8440 - val_greeks_loss: 0.0545 - val_ssr_loss: 0.6789 - lr: 0.0010\n",
      "Epoch 43/60\n",
      "2107/2107 [==============================] - 39s 18ms/step - loss: 7.6980 - greeks_loss: 0.1083 - ssr_loss: 0.7590 - val_loss: 6.8271 - val_greeks_loss: 0.0434 - val_ssr_loss: 0.6784 - lr: 0.0010\n",
      "Epoch 44/60\n",
      "2107/2107 [==============================] - 34s 16ms/step - loss: 7.6749 - greeks_loss: 0.1078 - ssr_loss: 0.7567 - val_loss: 6.9558 - val_greeks_loss: 0.0558 - val_ssr_loss: 0.6900 - lr: 0.0010\n",
      "Epoch 45/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.6579 - greeks_loss: 0.1089 - ssr_loss: 0.7549 - val_loss: 6.6890 - val_greeks_loss: 0.0558 - val_ssr_loss: 0.6633 - lr: 0.0010\n",
      "Epoch 46/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.6388 - greeks_loss: 0.1081 - ssr_loss: 0.7531 - val_loss: 6.8092 - val_greeks_loss: 0.0626 - val_ssr_loss: 0.6747 - lr: 0.0010\n",
      "Epoch 47/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.6156 - greeks_loss: 0.1082 - ssr_loss: 0.7507 - val_loss: 6.8812 - val_greeks_loss: 0.0512 - val_ssr_loss: 0.6830 - lr: 0.0010\n",
      "Epoch 48/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.5996 - greeks_loss: 0.1081 - ssr_loss: 0.7492 - val_loss: 6.8376 - val_greeks_loss: 0.0447 - val_ssr_loss: 0.6793 - lr: 0.0010\n",
      "Epoch 49/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.5821 - greeks_loss: 0.1081 - ssr_loss: 0.7474 - val_loss: 6.5251 - val_greeks_loss: 0.0517 - val_ssr_loss: 0.6473 - lr: 0.0010\n",
      "Epoch 50/60\n",
      "2107/2107 [==============================] - 33s 16ms/step - loss: 7.5587 - greeks_loss: 0.1083 - ssr_loss: 0.7450 - val_loss: 6.6438 - val_greeks_loss: 0.0617 - val_ssr_loss: 0.6582 - lr: 0.0010\n",
      "Epoch 51/60\n",
      "2107/2107 [==============================] - 35s 17ms/step - loss: 7.5448 - greeks_loss: 0.1084 - ssr_loss: 0.7436 - val_loss: 6.7859 - val_greeks_loss: 0.0530 - val_ssr_loss: 0.6733 - lr: 0.0010\n",
      "Epoch 52/60\n",
      "2107/2107 [==============================] - 33s 15ms/step - loss: 7.5271 - greeks_loss: 0.1081 - ssr_loss: 0.7419 - val_loss: 6.5882 - val_greeks_loss: 0.0511 - val_ssr_loss: 0.6537 - lr: 0.0010\n",
      "Epoch 53/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.5112 - greeks_loss: 0.1082 - ssr_loss: 0.7403 - val_loss: 6.4645 - val_greeks_loss: 0.0456 - val_ssr_loss: 0.6419 - lr: 0.0010\n",
      "Epoch 54/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.4936 - greeks_loss: 0.1090 - ssr_loss: 0.7385 - val_loss: 6.6180 - val_greeks_loss: 0.0467 - val_ssr_loss: 0.6571 - lr: 0.0010\n",
      "Epoch 55/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.4762 - greeks_loss: 0.1079 - ssr_loss: 0.7368 - val_loss: 6.4675 - val_greeks_loss: 0.0580 - val_ssr_loss: 0.6409 - lr: 0.0010\n",
      "Epoch 56/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.4605 - greeks_loss: 0.1091 - ssr_loss: 0.7351 - val_loss: 6.3872 - val_greeks_loss: 0.0492 - val_ssr_loss: 0.6338 - lr: 0.0010\n",
      "Epoch 57/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.4461 - greeks_loss: 0.1091 - ssr_loss: 0.7337 - val_loss: 6.3678 - val_greeks_loss: 0.0569 - val_ssr_loss: 0.6311 - lr: 0.0010\n",
      "Epoch 58/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.4314 - greeks_loss: 0.1092 - ssr_loss: 0.7322 - val_loss: 6.1870 - val_greeks_loss: 0.0500 - val_ssr_loss: 0.6137 - lr: 0.0010\n",
      "Epoch 59/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.4150 - greeks_loss: 0.1091 - ssr_loss: 0.7306 - val_loss: 6.1426 - val_greeks_loss: 0.0696 - val_ssr_loss: 0.6073 - lr: 0.0010\n",
      "Epoch 60/60\n",
      "2107/2107 [==============================] - 32s 15ms/step - loss: 7.3979 - greeks_loss: 0.1099 - ssr_loss: 0.7288 - val_loss: 6.1688 - val_greeks_loss: 0.0464 - val_ssr_loss: 0.6122 - lr: 0.0010\n",
      "Epoch 1/60\n",
      "2002/2002 [==============================] - 33s 15ms/step - loss: 10.4450 - greeks_loss: 0.3329 - ssr_loss: 1.0112 - val_loss: 11.1669 - val_greeks_loss: 0.1404 - val_ssr_loss: 1.1026 - lr: 0.0010\n",
      "Epoch 2/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 9.7738 - greeks_loss: 0.1691 - ssr_loss: 0.9605 - val_loss: 10.8390 - val_greeks_loss: 0.0985 - val_ssr_loss: 1.0741 - lr: 0.0010\n",
      "Epoch 3/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 9.6304 - greeks_loss: 0.1373 - ssr_loss: 0.9493 - val_loss: 10.7439 - val_greeks_loss: 0.1005 - val_ssr_loss: 1.0643 - lr: 0.0010\n",
      "Epoch 4/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 9.5282 - greeks_loss: 0.1232 - ssr_loss: 0.9405 - val_loss: 10.7268 - val_greeks_loss: 0.2148 - val_ssr_loss: 1.0512 - lr: 0.0010\n",
      "Epoch 5/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 9.4437 - greeks_loss: 0.1156 - ssr_loss: 0.9328 - val_loss: 10.6803 - val_greeks_loss: 0.0863 - val_ssr_loss: 1.0594 - lr: 0.0010\n",
      "Epoch 6/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 9.3615 - greeks_loss: 0.1092 - ssr_loss: 0.9252 - val_loss: 10.4274 - val_greeks_loss: 0.0660 - val_ssr_loss: 1.0361 - lr: 0.0010\n",
      "Epoch 7/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 9.2903 - greeks_loss: 0.1057 - ssr_loss: 0.9185 - val_loss: 10.1986 - val_greeks_loss: 0.0997 - val_ssr_loss: 1.0099 - lr: 0.0010\n",
      "Epoch 8/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 9.2254 - greeks_loss: 0.1036 - ssr_loss: 0.9122 - val_loss: 10.3093 - val_greeks_loss: 0.0599 - val_ssr_loss: 1.0249 - lr: 0.0010\n",
      "Epoch 9/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 9.1700 - greeks_loss: 0.1039 - ssr_loss: 0.9066 - val_loss: 10.1849 - val_greeks_loss: 0.0680 - val_ssr_loss: 1.0117 - lr: 0.0010\n",
      "Epoch 10/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 9.1131 - greeks_loss: 0.1009 - ssr_loss: 0.9012 - val_loss: 10.0524 - val_greeks_loss: 0.0648 - val_ssr_loss: 0.9988 - lr: 0.0010\n",
      "Epoch 11/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 9.0578 - greeks_loss: 0.1029 - ssr_loss: 0.8955 - val_loss: 9.8637 - val_greeks_loss: 0.1196 - val_ssr_loss: 0.9744 - lr: 0.0010\n",
      "Epoch 12/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 9.0057 - greeks_loss: 0.1008 - ssr_loss: 0.8905 - val_loss: 9.7758 - val_greeks_loss: 0.0638 - val_ssr_loss: 0.9712 - lr: 0.0010\n",
      "Epoch 13/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 8.9543 - greeks_loss: 0.1014 - ssr_loss: 0.8853 - val_loss: 9.7993 - val_greeks_loss: 0.0779 - val_ssr_loss: 0.9721 - lr: 0.0010\n",
      "Epoch 14/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 8.9060 - greeks_loss: 0.1016 - ssr_loss: 0.8804 - val_loss: 9.6626 - val_greeks_loss: 0.0520 - val_ssr_loss: 0.9611 - lr: 0.0010\n",
      "Epoch 15/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 8.8601 - greeks_loss: 0.1016 - ssr_loss: 0.8759 - val_loss: 9.7208 - val_greeks_loss: 0.0619 - val_ssr_loss: 0.9659 - lr: 0.0010\n",
      "Epoch 16/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 8.8114 - greeks_loss: 0.1014 - ssr_loss: 0.8710 - val_loss: 9.3476 - val_greeks_loss: 0.0556 - val_ssr_loss: 0.9292 - lr: 0.0010\n",
      "Epoch 17/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.7690 - greeks_loss: 0.1017 - ssr_loss: 0.8667 - val_loss: 9.3739 - val_greeks_loss: 0.0649 - val_ssr_loss: 0.9309 - lr: 0.0010\n",
      "Epoch 18/60\n",
      "2002/2002 [==============================] - 35s 17ms/step - loss: 8.7172 - greeks_loss: 0.1015 - ssr_loss: 0.8616 - val_loss: 9.3411 - val_greeks_loss: 0.0641 - val_ssr_loss: 0.9277 - lr: 0.0010\n",
      "Epoch 19/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.6788 - greeks_loss: 0.1034 - ssr_loss: 0.8575 - val_loss: 9.1668 - val_greeks_loss: 0.0598 - val_ssr_loss: 0.9107 - lr: 0.0010\n",
      "Epoch 20/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 8.6331 - greeks_loss: 0.1023 - ssr_loss: 0.8531 - val_loss: 9.1749 - val_greeks_loss: 0.0699 - val_ssr_loss: 0.9105 - lr: 0.0010\n",
      "Epoch 21/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.5946 - greeks_loss: 0.1050 - ssr_loss: 0.8490 - val_loss: 9.0896 - val_greeks_loss: 0.0645 - val_ssr_loss: 0.9025 - lr: 0.0010\n",
      "Epoch 22/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.5562 - greeks_loss: 0.1039 - ssr_loss: 0.8452 - val_loss: 9.1192 - val_greeks_loss: 0.0830 - val_ssr_loss: 0.9036 - lr: 0.0010\n",
      "Epoch 23/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 8.5184 - greeks_loss: 0.1033 - ssr_loss: 0.8415 - val_loss: 8.7823 - val_greeks_loss: 0.0758 - val_ssr_loss: 0.8707 - lr: 0.0010\n",
      "Epoch 24/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.4814 - greeks_loss: 0.1032 - ssr_loss: 0.8378 - val_loss: 9.0308 - val_greeks_loss: 0.0663 - val_ssr_loss: 0.8965 - lr: 0.0010\n",
      "Epoch 25/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 8.4460 - greeks_loss: 0.1033 - ssr_loss: 0.8343 - val_loss: 8.7765 - val_greeks_loss: 0.0719 - val_ssr_loss: 0.8705 - lr: 0.0010\n",
      "Epoch 26/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.4177 - greeks_loss: 0.1046 - ssr_loss: 0.8313 - val_loss: 8.8745 - val_greeks_loss: 0.0544 - val_ssr_loss: 0.8820 - lr: 0.0010\n",
      "Epoch 27/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.3823 - greeks_loss: 0.1041 - ssr_loss: 0.8278 - val_loss: 8.6101 - val_greeks_loss: 0.0537 - val_ssr_loss: 0.8556 - lr: 0.0010\n",
      "Epoch 28/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.3531 - greeks_loss: 0.1046 - ssr_loss: 0.8249 - val_loss: 8.6731 - val_greeks_loss: 0.0726 - val_ssr_loss: 0.8601 - lr: 0.0010\n",
      "Epoch 29/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 8.3267 - greeks_loss: 0.1047 - ssr_loss: 0.8222 - val_loss: 8.3205 - val_greeks_loss: 0.0811 - val_ssr_loss: 0.8239 - lr: 0.0010\n",
      "Epoch 30/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.2951 - greeks_loss: 0.1056 - ssr_loss: 0.8190 - val_loss: 8.2593 - val_greeks_loss: 0.0673 - val_ssr_loss: 0.8192 - lr: 0.0010\n",
      "Epoch 31/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.2632 - greeks_loss: 0.1041 - ssr_loss: 0.8159 - val_loss: 8.2431 - val_greeks_loss: 0.0534 - val_ssr_loss: 0.8190 - lr: 0.0010\n",
      "Epoch 32/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.2385 - greeks_loss: 0.1053 - ssr_loss: 0.8133 - val_loss: 8.3154 - val_greeks_loss: 0.0642 - val_ssr_loss: 0.8251 - lr: 0.0010\n",
      "Epoch 33/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.2093 - greeks_loss: 0.1045 - ssr_loss: 0.8105 - val_loss: 8.2734 - val_greeks_loss: 0.0599 - val_ssr_loss: 0.8213 - lr: 0.0010\n",
      "Epoch 34/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.1811 - greeks_loss: 0.1057 - ssr_loss: 0.8075 - val_loss: 8.1194 - val_greeks_loss: 0.0645 - val_ssr_loss: 0.8055 - lr: 0.0010\n",
      "Epoch 35/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.1540 - greeks_loss: 0.1052 - ssr_loss: 0.8049 - val_loss: 8.1486 - val_greeks_loss: 0.0762 - val_ssr_loss: 0.8072 - lr: 0.0010\n",
      "Epoch 36/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.1344 - greeks_loss: 0.1059 - ssr_loss: 0.8029 - val_loss: 8.0307 - val_greeks_loss: 0.0762 - val_ssr_loss: 0.7955 - lr: 0.0010\n",
      "Epoch 37/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.1038 - greeks_loss: 0.1051 - ssr_loss: 0.7999 - val_loss: 7.8084 - val_greeks_loss: 0.0545 - val_ssr_loss: 0.7754 - lr: 0.0010\n",
      "Epoch 38/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.0875 - greeks_loss: 0.1067 - ssr_loss: 0.7981 - val_loss: 7.7774 - val_greeks_loss: 0.0643 - val_ssr_loss: 0.7713 - lr: 0.0010\n",
      "Epoch 39/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 8.0586 - greeks_loss: 0.1065 - ssr_loss: 0.7952 - val_loss: 7.7876 - val_greeks_loss: 0.0643 - val_ssr_loss: 0.7723 - lr: 0.0010\n",
      "Epoch 40/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.0348 - greeks_loss: 0.1061 - ssr_loss: 0.7929 - val_loss: 7.5565 - val_greeks_loss: 0.0614 - val_ssr_loss: 0.7495 - lr: 0.0010\n",
      "Epoch 41/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 8.0161 - greeks_loss: 0.1071 - ssr_loss: 0.7909 - val_loss: 7.6433 - val_greeks_loss: 0.0601 - val_ssr_loss: 0.7583 - lr: 0.0010\n",
      "Epoch 42/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 7.9898 - greeks_loss: 0.1062 - ssr_loss: 0.7884 - val_loss: 7.7579 - val_greeks_loss: 0.0629 - val_ssr_loss: 0.7695 - lr: 0.0010\n",
      "Epoch 43/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 7.9696 - greeks_loss: 0.1063 - ssr_loss: 0.7863 - val_loss: 7.4970 - val_greeks_loss: 0.0681 - val_ssr_loss: 0.7429 - lr: 0.0010\n",
      "Epoch 44/60\n",
      "2002/2002 [==============================] - 30s 15ms/step - loss: 7.9528 - greeks_loss: 0.1070 - ssr_loss: 0.7846 - val_loss: 7.4544 - val_greeks_loss: 0.0845 - val_ssr_loss: 0.7370 - lr: 0.0010\n",
      "Epoch 45/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.9285 - greeks_loss: 0.1057 - ssr_loss: 0.7823 - val_loss: 7.2172 - val_greeks_loss: 0.0605 - val_ssr_loss: 0.7157 - lr: 0.0010\n",
      "Epoch 46/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.9073 - greeks_loss: 0.1065 - ssr_loss: 0.7801 - val_loss: 7.3101 - val_greeks_loss: 0.0529 - val_ssr_loss: 0.7257 - lr: 0.0010\n",
      "Epoch 47/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.8912 - greeks_loss: 0.1072 - ssr_loss: 0.7784 - val_loss: 7.1284 - val_greeks_loss: 0.0598 - val_ssr_loss: 0.7069 - lr: 0.0010\n",
      "Epoch 48/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.8710 - greeks_loss: 0.1072 - ssr_loss: 0.7764 - val_loss: 7.1252 - val_greeks_loss: 0.0573 - val_ssr_loss: 0.7068 - lr: 0.0010\n",
      "Epoch 49/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.8533 - greeks_loss: 0.1073 - ssr_loss: 0.7746 - val_loss: 6.9991 - val_greeks_loss: 0.0662 - val_ssr_loss: 0.6933 - lr: 0.0010\n",
      "Epoch 50/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.8341 - greeks_loss: 0.1073 - ssr_loss: 0.7727 - val_loss: 6.9935 - val_greeks_loss: 0.0535 - val_ssr_loss: 0.6940 - lr: 0.0010\n",
      "Epoch 51/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.8126 - greeks_loss: 0.1066 - ssr_loss: 0.7706 - val_loss: 7.1586 - val_greeks_loss: 0.0844 - val_ssr_loss: 0.7074 - lr: 0.0010\n",
      "Epoch 52/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.7942 - greeks_loss: 0.1062 - ssr_loss: 0.7688 - val_loss: 6.9433 - val_greeks_loss: 0.0569 - val_ssr_loss: 0.6886 - lr: 0.0010\n",
      "Epoch 53/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.7806 - greeks_loss: 0.1068 - ssr_loss: 0.7674 - val_loss: 6.9620 - val_greeks_loss: 0.0748 - val_ssr_loss: 0.6887 - lr: 0.0010\n",
      "Epoch 54/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.7640 - greeks_loss: 0.1074 - ssr_loss: 0.7657 - val_loss: 6.8612 - val_greeks_loss: 0.1180 - val_ssr_loss: 0.6743 - lr: 0.0010\n",
      "Epoch 55/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.7455 - greeks_loss: 0.1073 - ssr_loss: 0.7638 - val_loss: 6.6962 - val_greeks_loss: 0.0550 - val_ssr_loss: 0.6641 - lr: 0.0010\n",
      "Epoch 56/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.7274 - greeks_loss: 0.1074 - ssr_loss: 0.7620 - val_loss: 6.7322 - val_greeks_loss: 0.0553 - val_ssr_loss: 0.6677 - lr: 0.0010\n",
      "Epoch 57/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.7134 - greeks_loss: 0.1065 - ssr_loss: 0.7607 - val_loss: 6.6585 - val_greeks_loss: 0.0603 - val_ssr_loss: 0.6598 - lr: 0.0010\n",
      "Epoch 58/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.6994 - greeks_loss: 0.1072 - ssr_loss: 0.7592 - val_loss: 6.7223 - val_greeks_loss: 0.0603 - val_ssr_loss: 0.6662 - lr: 0.0010\n",
      "Epoch 59/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.6859 - greeks_loss: 0.1077 - ssr_loss: 0.7578 - val_loss: 6.7495 - val_greeks_loss: 0.0549 - val_ssr_loss: 0.6695 - lr: 0.0010\n",
      "Epoch 60/60\n",
      "2002/2002 [==============================] - 31s 15ms/step - loss: 7.6648 - greeks_loss: 0.1071 - ssr_loss: 0.7558 - val_loss: 6.6956 - val_greeks_loss: 0.0836 - val_ssr_loss: 0.6612 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ae21719880>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5) Train multi-task network\n",
    "# ----------------------------------------------------------------------------\n",
    "call_m = build_mlp_mt(cXtr.shape[1])\n",
    "call_m.fit(cXtr, [cYtr[:,:5], cYtr[:,5:]], validation_data=(cXva, [cYva[:,:5], cYva[:,5:]]),\n",
    "           epochs=60, batch_size=4096, callbacks=CB, verbose=1)\n",
    "put_m  = build_mlp_mt(pXtr.shape[1])\n",
    "put_m.fit(pXtr,  [pYtr[:,:5], pYtr[:,5:]], validation_data=(pXva, [pYva[:,:5], pYva[:,5:]]),\n",
    "          epochs=60, batch_size=4096, callbacks=CB, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4df311b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4128/4128 [==============================] - 16s 4ms/step\n",
      "\n",
      "CALL MODEL Performance:\n",
      "mid_price   MSE=5.828734  MAE=1.495236  R²=0.9711\n",
      "delta       MSE=0.006409  MAE=0.050326  R²=0.9402\n",
      "gamma       MSE=0.008975  MAE=0.020366  R²=0.5222\n",
      "vega        MSE=13.010059  MAE=2.538499  R²=0.8167\n",
      "theta       MSE=134.431546  MAE=3.626314  R²=0.8678\n",
      "SSR         MSE=3.916497  MAE=1.351317  R²=0.4609\n",
      "3922/3922 [==============================] - 15s 4ms/step\n",
      "\n",
      "PUT MODEL Performance:\n",
      "mid_price   MSE=8.147453  MAE=1.742968  R²=0.9582\n",
      "delta       MSE=0.006050  MAE=0.059347  R²=0.9451\n",
      "gamma       MSE=0.008939  MAE=0.024719  R²=0.5937\n",
      "vega        MSE=17.140207  MAE=3.087860  R²=0.8127\n",
      "theta       MSE=222.905080  MAE=5.047914  R²=0.7944\n",
      "SSR         MSE=4.220377  MAE=1.398403  R²=0.4161\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6) Evaluate including SSR with detailed reports\n",
    "# ----------------------------------------------------------------------------\n",
    "def evaluate_mt(model, X, Y, gs, ss, tag):\n",
    "    pred_g, pred_s = model.predict(X, verbose=1)\n",
    "    true_g = gs.inverse_transform(Y[:,:5])\n",
    "    true_s = ss.inverse_transform(Y[:,5:])\n",
    "    pred_g = gs.inverse_transform(pred_g)\n",
    "    pred_s = ss.inverse_transform(pred_s)\n",
    "\n",
    "    print(f\"\\n{tag} MODEL Performance:\")\n",
    "    for i, name in enumerate(GREEKS):\n",
    "        mse = mean_squared_error(true_g[:,i], pred_g[:,i])\n",
    "        mae = mean_absolute_error(true_g[:,i], pred_g[:,i])\n",
    "        r2  = r2_score(true_g[:,i], pred_g[:,i])\n",
    "        print(f\"{name:10s}  MSE={mse:.6f}  MAE={mae:.6f}  R²={r2:.4f}\")\n",
    "    mse_s = mean_squared_error(true_s[:,0], pred_s[:,0])\n",
    "    mae_s = mean_absolute_error(true_s[:,0], pred_s[:,0])\n",
    "    r2_s  = r2_score(true_s[:,0], pred_s[:,0])\n",
    "    print(f\"{'SSR':10s}  MSE={mse_s:.6f}  MAE={mae_s:.6f}  R²={r2_s:.4f}\")\n",
    "\n",
    "evaluate_mt(call_m, cXte, cYte, greek_scaler, ssr_scaler, 'CALL')\n",
    "evaluate_mt(put_m,  pXte, pYte, greek_scaler, ssr_scaler, 'PUT')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
