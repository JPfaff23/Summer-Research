{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca9e8308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROBE] Training…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\autograd\\graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:181.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1/2000] train=3.072e+00  val=6.092e-01\n",
      "[  500/2000] train=6.420e-01  val=6.390e-01\n",
      "[ 1000/2000] train=5.745e-01  val=5.707e-01\n",
      "[ 1500/2000] train=5.715e-01  val=5.545e-01\n",
      "[ 2000/2000] train=5.689e-01  val=5.515e-01\n",
      "[PROBE] Done in 558.9s\n",
      "\n",
      "[PROBE] Test-set R² and slopes\n",
      "     Price | R²=0.961443  slope=0.971749  intercept=6.281e-03\n",
      "   Delta_0 | R²=0.912088  slope=0.913034  intercept=1.221e-02\n",
      "   Delta_1 | R²=0.912275  slope=0.917097  intercept=1.197e-02\n",
      "   Delta_2 | R²=0.912716  slope=0.920972  intercept=1.247e-02\n",
      "    Vega_0 | R²=-0.020745  slope=0.003393  intercept=-6.038e-02\n",
      "    Vega_1 | R²=-0.012030  slope=0.002966  intercept=-5.221e-02\n",
      "    Vega_2 | R²=-0.014062  slope=0.003082  intercept=-5.284e-02\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MICRO PROBE TRAIN (fast, self-contained)\n",
    "# =========================\n",
    "import time, torch, numpy as np\n",
    "import torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# --- Config ---\n",
    "PROBE_SEED          = 123\n",
    "PROBE_WIDTH         = 192\n",
    "PROBE_LAYERS        = 4\n",
    "PROBE_BATCH         = 8192\n",
    "PROBE_UPDATES       = 2000        # ~1–2 minutes; bump to 4000 if you can\n",
    "PROBE_LR            = 1e-3\n",
    "PROBE_WDECAY        = 1e-6\n",
    "PROBE_LAMBDA_MAX    = 0.5\n",
    "PROBE_WARMUP_STEPS  = 400\n",
    "DEVICE              = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Use your loaded dfs/columns ---\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "y_p = df[TARGET_PRICE].values.astype(np.float32)\n",
    "y_d = df[delta_cols].values.astype(np.float32)\n",
    "y_v = df[vega_cols].values.astype(np.float32)\n",
    "\n",
    "# train/val split\n",
    "idx = np.arange(len(df))\n",
    "tr_idx, val_idx = train_test_split(idx, test_size=0.01, random_state=PROBE_SEED, shuffle=True)\n",
    "X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "p_tr, p_val = y_p[tr_idx], y_p[val_idx]\n",
    "d_tr, d_val = y_d[tr_idx], y_d[val_idx]\n",
    "v_tr, v_val = y_v[tr_idx], y_v[val_idx]\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(X_tr),  torch.from_numpy(p_tr),\n",
    "                         torch.from_numpy(d_tr),  torch.from_numpy(v_tr))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(p_val),\n",
    "                         torch.from_numpy(d_val), torch.from_numpy(v_val))\n",
    "\n",
    "# weighting\n",
    "d_std = np.maximum(d_tr.std(axis=0), 1e-12)\n",
    "v_std = np.maximum(v_tr.std(axis=0), 1e-12)\n",
    "w_delta = torch.tensor(1.0/(d_std**2), dtype=torch.float32, device=DEVICE)\n",
    "w_vega  = torch.tensor(1.0/(v_std**2), dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "# model\n",
    "class ProbeNet(nn.Module):\n",
    "    def __init__(self, in_dim, width=PROBE_WIDTH, layers=PROBE_LAYERS):\n",
    "        super().__init__()\n",
    "        blocks = [nn.Linear(in_dim, width), nn.Softplus()]\n",
    "        for _ in range(layers - 1):\n",
    "            blocks += [nn.Linear(width, width), nn.Softplus()]\n",
    "        blocks += [nn.Linear(width, 1)]\n",
    "        self.net = nn.Sequential(*blocks)\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "    def forward(self, x): return self.net(x).squeeze(1)\n",
    "\n",
    "def weighted_mse(pred, true, w):\n",
    "    return ((pred - true)**2 * w).mean()\n",
    "\n",
    "# indices for autograd greeks\n",
    "def require_feature(name: str) -> int: return feature_cols.index(name)\n",
    "delta_idx = [require_feature(f\"S0_{i}/K\") for i in range(3)]\n",
    "vega_idx  = [require_feature(f\"sigma_{i}\") for i in range(3)]\n",
    "\n",
    "# --- train briefly ---\n",
    "torch.manual_seed(PROBE_SEED); np.random.seed(PROBE_SEED)\n",
    "model = ProbeNet(len(feature_cols)).to(DEVICE)\n",
    "opt   = optim.Adam(model.parameters(), lr=PROBE_LR, weight_decay=PROBE_WDECAY)\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=PROBE_UPDATES, eta_min=PROBE_LR/50)\n",
    "mse   = nn.MSELoss()\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=PROBE_BATCH, shuffle=True, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=PROBE_BATCH, shuffle=False, pin_memory=True)\n",
    "\n",
    "def lam_sob(step): return PROBE_LAMBDA_MAX*min(1.0, step/PROBE_WARMUP_STEPS)\n",
    "\n",
    "print(\"\\n[PROBE] Training…\")\n",
    "t0 = time.time()\n",
    "it = iter(train_loader)\n",
    "for step in range(1, PROBE_UPDATES+1):\n",
    "    try: Xb, p_true, d_true, v_true = next(it)\n",
    "    except StopIteration:\n",
    "        it = iter(train_loader); Xb, p_true, d_true, v_true = next(it)\n",
    "    Xb = Xb.to(DEVICE).requires_grad_(True)\n",
    "    p_true, d_true, v_true = [t.to(DEVICE) for t in (p_true, d_true, v_true)]\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    p_pred = model(Xb)\n",
    "    loss_p = mse(p_pred, p_true)\n",
    "    g = torch.autograd.grad(p_pred, Xb, grad_outputs=torch.ones_like(p_pred), create_graph=True)[0]\n",
    "    d_pred = g[:, delta_idx]; v_pred = g[:, vega_idx]\n",
    "    loss = loss_p + lam_sob(step)*(weighted_mse(d_pred,d_true,w_delta)+weighted_mse(v_pred,v_true,w_vega))\n",
    "    loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    opt.step(); sched.step()\n",
    "\n",
    "    if step % 500 == 0 or step==1 or step==PROBE_UPDATES:\n",
    "        model.eval(); tot=cnt=0\n",
    "        for Xv,pv,dv,vv in val_loader:\n",
    "            Xv = Xv.to(DEVICE).requires_grad_(True); pv,dv,vv=[t.to(DEVICE) for t in (pv,dv,vv)]\n",
    "            pp=model(Xv); lp=mse(pp,pv)\n",
    "            gv=torch.autograd.grad(pp,Xv,grad_outputs=torch.ones_like(pp))[0]\n",
    "            ld=weighted_mse(gv[:,delta_idx],dv,w_delta); lv=weighted_mse(gv[:,vega_idx],vv,w_vega)\n",
    "            tot+=(lp+lam_sob(step)*(ld+lv)).item()*Xv.size(0); cnt+=Xv.size(0)\n",
    "        print(f\"[{step:>5}/{PROBE_UPDATES}] train={loss.item():.3e}  val={tot/cnt:.3e}\")\n",
    "        model.train()\n",
    "print(f\"[PROBE] Done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# --- evaluate on TEST ---\n",
    "X_test = torch.from_numpy(test_df[feature_cols].values.astype(np.float32)).to(DEVICE).requires_grad_(True)\n",
    "p_pred = model(X_test)\n",
    "g_test = torch.autograd.grad(p_pred, X_test, grad_outputs=torch.ones_like(p_pred))[0]\n",
    "price_pred = p_pred.detach().cpu().numpy()\n",
    "delta_pred = g_test[:, delta_idx].detach().cpu().numpy()\n",
    "vega_pred  = g_test[:, vega_idx].detach().cpu().numpy()\n",
    "\n",
    "def r2_slope(pred, true, name):\n",
    "    if np.std(true)==0 or np.std(pred)==0:\n",
    "        print(f\"{name:>10s} | R²=nan\")\n",
    "        return\n",
    "    r2=r2_score(true.ravel(),pred.ravel()); a1,a0=np.polyfit(true.ravel(),pred.ravel(),1)\n",
    "    print(f\"{name:>10s} | R²={r2:.6f}  slope={a1:.6f}  intercept={a0:.3e}\")\n",
    "\n",
    "print(\"\\n[PROBE] Test-set R² and slopes\")\n",
    "r2_slope(price_pred, test_df[TARGET_PRICE].values, \"Price\")\n",
    "for i in range(3): r2_slope(delta_pred[:,i], test_df[delta_cols].values[:,i], f\"Delta_{i}\")\n",
    "for i in range(3): r2_slope(vega_pred[:,i],  test_df[vega_cols].values[:,i],  f\"Vega_{i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f89d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AD~FD vega_dim0] Corr=0.999472, slope(AD~FD)=1.000211  -> should be ~1 & ~1\n",
      "\n",
      "[VEGA UNIT SWEEP] Comparing predicted vega under different mappings vs labels:\n",
      "        raw (∂p̂/∂σ) | R²=-0.015418  slope=0.003150  intercept=-5.518e-02\n",
      "   variance (1/(2σ)) | R²=-0.017941  slope=0.001680  intercept=-1.017e-01\n",
      "  percent-vol (/100) | R²=-0.022594  slope=0.000031  intercept=-5.518e-04\n",
      "  percent-vol (*100) | R²=0.189626  slope=0.314996  intercept=-5.518e+00\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# VEGA DIAGNOSTIC: AD~FD + unit-mapping sweep\n",
    "# (Run right now; uses your `model`, `test_df`, `feature_cols`, `delta_cols`, `vega_cols`)\n",
    "# =========================\n",
    "import numpy as np, torch\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1) AD vs FD sanity (vega_dim0) ---\n",
    "n = min(len(test_df), 2048)\n",
    "Xb_np = test_df[feature_cols].values[:n].astype(np.float32)\n",
    "Xb    = torch.from_numpy(Xb_np).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    p0 = model(Xb).detach()\n",
    "\n",
    "# choose one vega dim to sanity check\n",
    "vega_cols_feat = [c for c in feature_cols if c.startswith(\"sigma_\")]\n",
    "assert len(vega_cols_feat) >= 1, \"No sigma_* features found.\"\n",
    "# Use the same vega_idx mapping you already have:\n",
    "jV = feature_cols.index(\"sigma_0\")\n",
    "\n",
    "eps = 1e-4\n",
    "Xe = Xb_np.copy(); Xe[:, jV] += eps\n",
    "with torch.no_grad(): p_eps = model(torch.from_numpy(Xe).to(DEVICE)).detach()\n",
    "vega_fd = (p_eps - p0).cpu().numpy() / eps\n",
    "\n",
    "Xreq = Xb.clone().requires_grad_(True)\n",
    "pp   = model(Xreq)\n",
    "g    = torch.autograd.grad(pp, Xreq, grad_outputs=torch.ones_like(pp))[0]\n",
    "vega_ad = g[:, jV].detach().cpu().numpy()\n",
    "\n",
    "cV = np.corrcoef(vega_ad.ravel(), vega_fd.ravel())[0,1]\n",
    "sV = np.polyfit(vega_fd.ravel(),  vega_ad.ravel(), 1)[0]\n",
    "print(f\"[AD~FD vega_dim0] Corr={cV:.6f}, slope(AD~FD)={sV:.6f}  -> should be ~1 & ~1\")\n",
    "\n",
    "# --- 2) Unit mapping sweep vs labels (all three vegas) ---\n",
    "yV = test_df[vega_cols].values.astype(np.float32)\n",
    "\n",
    "# get full predictions once\n",
    "X_all = torch.from_numpy(test_df[feature_cols].values.astype(np.float32)).to(DEVICE).requires_grad_(True)\n",
    "p_all = model(X_all)\n",
    "g_all = torch.autograd.grad(p_all, X_all, grad_outputs=torch.ones_like(p_all))[0]\n",
    "vega_pred = g_all[:, [feature_cols.index(\"sigma_0\"),\n",
    "                      feature_cols.index(\"sigma_1\"),\n",
    "                      feature_cols.index(\"sigma_2\")]].detach().cpu().numpy()\n",
    "\n",
    "# helpers (pull K, sigma if available)\n",
    "# If K is not explicit, we can often reconstruct from S0_i and S0_i/K if both exist; otherwise skip K tests.\n",
    "def col_ok(name): return name in test_df.columns\n",
    "have_K = col_ok(\"K\") or all(col_ok(f\"S0_{i}\") and col_ok(f\"S0_{i}/K\") for i in range(3))\n",
    "if have_K and not col_ok(\"K\"):\n",
    "    # reconstruct K from leg 0 if available (S0_0 / (S0_0/K))\n",
    "    K = (test_df[\"S0_0\"] / test_df[\"S0_0/K\"]).values\n",
    "elif col_ok(\"K\"):\n",
    "    K = test_df[\"K\"].values\n",
    "else:\n",
    "    K = None\n",
    "\n",
    "sig = test_df[[\"sigma_0\",\"sigma_1\",\"sigma_2\"]].values if all(col_ok(f\"sigma_{i}\") for i in range(3)) else None\n",
    "\n",
    "def eval_map(name, pred):\n",
    "    r2 = r2_score(yV.ravel(), pred.ravel()) if (np.std(yV)>0 and np.std(pred)>0) else np.nan\n",
    "    a1, a0 = np.polyfit(yV.ravel(), pred.ravel(), 1)\n",
    "    print(f\"{name:>20s} | R²={r2:.6f}  slope={a1:.6f}  intercept={a0:.3e}\")\n",
    "\n",
    "print(\"\\n[VEGA UNIT SWEEP] Comparing predicted vega under different mappings vs labels:\")\n",
    "eval_map(\"raw (∂p̂/∂σ)\", vega_pred)  # p̂ = price/K, σ in decimals\n",
    "\n",
    "if K is not None:\n",
    "    eval_map(\"raw * K\", vega_pred * K[:, None])\n",
    "    eval_map(\"raw / K\", vega_pred / K[:, None])\n",
    "\n",
    "if sig is not None:\n",
    "    # If labels are wrt variance v=σ^2, then ∂p̂/∂v = (1/(2σ))*∂p̂/∂σ  (when σ>0)\n",
    "    eval_map(\"variance (1/(2σ))\", vega_pred / (2.0 * np.clip(sig, 1e-8, None)))\n",
    "    # If labels used percent-vol (σ% = 100*σ): ∂p̂/∂(σ%) = (1/100)*∂p̂/∂σ\n",
    "    eval_map(\"percent-vol (/100)\", vega_pred / 100.0)\n",
    "    eval_map(\"percent-vol (*100)\", vega_pred * 100.0)\n",
    "\n",
    "# Also try price vs price/K mismatch:\n",
    "# If labels are ∂Price/∂σ but model gives ∂(Price/K)/∂σ, multiply by K\n",
    "if K is not None:\n",
    "    eval_map(\"price-level (K * raw)\", K[:, None] * vega_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e815b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      " • sigma_0\n",
      " • sigma_1\n",
      " • sigma_2\n",
      " • corr_0_1\n",
      " • corr_0_2\n",
      " • corr_1_2\n",
      " • r\n",
      " • T\n",
      " • delta_0\n",
      " • delta_1\n",
      " • delta_2\n",
      " • vega_0\n",
      " • vega_1\n",
      " • vega_2\n",
      " • delta_aad_0\n",
      " • delta_aad_1\n",
      " • delta_aad_2\n",
      " • vega_aad_0\n",
      " • vega_aad_1\n",
      " • vega_aad_2\n",
      " • S0_0/K\n",
      " • S0_1/K\n",
      " • S0_2/K\n",
      " • price/k\n",
      "\n",
      "Rows: 4,821,510 | Columns: 24\n",
      "\n",
      "Sample rows:\n",
      "    sigma_0   sigma_1   sigma_2  corr_0_1  corr_0_2  corr_1_2     r         T  \\\n",
      "0  0.167014  0.166996  0.093563  0.595795  0.267429  0.404955  0.03  2.285714   \n",
      "1  0.694955  0.560231  0.387874  0.040739  0.310864  0.304059  0.03  1.285714   \n",
      "2  0.741406  0.116369  0.196987  0.542316  0.920858  0.695307  0.03  6.349206   \n",
      "3  0.642632  0.504470  0.744726  0.323487  0.165483  0.568074  0.03  6.670635   \n",
      "4  0.201289  0.721823  0.406528  0.160354  0.201242  0.880389  0.03  4.063492   \n",
      "\n",
      "    delta_0   delta_1  ...  delta_aad_0  delta_aad_1  delta_aad_2  vega_aad_0  \\\n",
      "0  0.059344  0.840880  ...     0.059342     0.840888     0.030561  -18.786872   \n",
      "1  0.170168  0.093786  ...     0.170172     0.093783     0.033695   -2.253025   \n",
      "2  0.033292  0.062233  ...     0.033293     0.062234     0.453972  -98.223286   \n",
      "3  0.039880  0.010080  ...     0.039880     0.010080     0.051811   -7.954524   \n",
      "4  0.158407  0.056134  ...     0.158406     0.056136     0.007160    9.467053   \n",
      "\n",
      "   vega_aad_1  vega_aad_2    S0_0/K    S0_1/K    S0_2/K   price/k  \n",
      "0  -23.880916   -4.509690  2.113525  1.538592  2.279245  0.585722  \n",
      "1   -8.445707   -4.588809  1.123009  1.628684  1.969492  0.129716  \n",
      "2   13.775075   80.360072  6.613788  2.994628  1.839179  0.770696  \n",
      "3   -2.906846   -8.292219  1.436701  2.266309  1.047744  0.074241  \n",
      "4   -7.912611   -1.140586  1.116444  1.081179  1.529182  0.076101  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load just the schema / small sample\n",
    "df = pd.read_parquet(\"Train_Clean.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# Show all column names\n",
    "print(\"Columns:\")\n",
    "for col in df.columns:\n",
    "    print(\" •\", col)\n",
    "\n",
    "# Optional: show shape\n",
    "print(f\"\\nRows: {len(df):,} | Columns: {len(df.columns)}\")\n",
    "\n",
    "# Peek at first 5 rows\n",
    "print(\"\\nSample rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963ba019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Train_Clean.parquet: 4,821,510 rows, 24 cols\n",
      "Saved cleaned file: Train_Clean_Cleaned.parquet with 18 cols\n",
      "Loaded Test_Clean.parquet: 48,224 rows, 24 cols\n",
      "Saved cleaned file: Test_Clean_Cleaned.parquet with 18 cols\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_parquet(in_path, out_path):\n",
    "    # Load file\n",
    "    df = pd.read_parquet(in_path, engine=\"pyarrow\")\n",
    "    print(f\"Loaded {in_path}: {df.shape[0]:,} rows, {df.shape[1]} cols\")\n",
    "\n",
    "    # 1) Drop FD Greek columns\n",
    "    drop_cols = [f\"delta_{i}\" for i in range(3)] + [f\"vega_{i}\" for i in range(3)]\n",
    "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    # 2) Rename AAD Greeks → canonical names\n",
    "    rename_map = {f\"delta_aad_{i}\": f\"delta_{i}\" for i in range(3)}\n",
    "    rename_map.update({f\"vega_aad_{i}\": f\"vega_{i}\" for i in range(3)})\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # 3) Save cleaned file\n",
    "    df.to_parquet(out_path, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Saved cleaned file: {out_path} with {df.shape[1]} cols\")\n",
    "\n",
    "# Clean both train & test\n",
    "clean_parquet(\"Train_Clean.parquet\", \"Train_Clean_Cleaned.parquet\")\n",
    "clean_parquet(\"Test_Clean.parquet\",  \"Test_Clean_Cleaned.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e2c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loaded Train_Clean.parquet → shape (4821510, 24) ===\n",
      "\n",
      "[Schema]\n",
      "Missing expected columns : None ✅\n",
      "Unexpected extra columns : ['delta_aad_0', 'delta_aad_1', 'delta_aad_2', 'vega_aad_0', 'vega_aad_1', 'vega_aad_2']\n",
      "RAW columns still present: None ✅\n",
      "\n",
      "[NA / Inf]\n",
      "NA counts: None ✅\n",
      "Any ±Inf?  No ✅\n",
      "\n",
      "[Range checks (from your data generator)]\n",
      "sigma_0 in [0.05,0.8]: 0.000% ✅\n",
      "sigma_1 in [0.05,0.8]: 0.000% ✅\n",
      "sigma_2 in [0.05,0.8]: 0.000% ✅\n",
      "r == 0.03             : 0.000% ✅\n",
      "T in [0.00397,7.34]   : 1.656% ⚠️\n",
      "corr_0_1 in [-1,1]       : 0.000% ✅\n",
      "corr_0_2 in [-1,1]       : 0.000% ✅\n",
      "corr_1_2 in [-1,1]       : 0.000% ✅\n",
      "  S0_0/K > 0            : 0.000% ✅\n",
      "  S0_1/K > 0            : 0.000% ✅\n",
      "  S0_2/K > 0            : 0.000% ✅\n",
      " price/k ≥ 1e-6       : 0.000% ✅\n",
      "\n",
      "[Delta sanity]\n",
      "each delta_i in [0,1] : 99.23% ⚠️\n",
      "sum(delta_i) ≤ 1      : 99.17% ⚠️\n",
      "\n",
      "[Vega scaling sanity]\n",
      "            mean       std       min        1%       50%       99%       max\n",
      "vega_0 -0.031974  0.191593 -3.043398 -0.636702 -0.006985  0.520611  2.357525\n",
      "vega_1 -0.026813  0.193278 -3.403978 -0.629408 -0.004551  0.544180  2.289713\n",
      "vega_2 -0.026748  0.193138 -5.289182 -0.629348 -0.004577  0.543069  2.778866\n",
      "Rows with any |vega| > 20: 0.000% ✅ (scaled)\n",
      "\n",
      "[Quick .describe() of all features/targets]\n",
      "              count      mean           std       min        1%       50%  \\\n",
      "sigma_0   4821510.0  0.427069  2.159877e-01  0.050000  0.057832  0.427782   \n",
      "sigma_1   4821510.0  0.427130  2.159136e-01  0.050000  0.057878  0.427814   \n",
      "sigma_2   4821510.0  0.426887  2.158496e-01  0.050000  0.057932  0.427566   \n",
      "corr_0_1  4821510.0  0.428565  3.193790e-01 -0.929757 -0.411541  0.471035   \n",
      "corr_0_2  4821510.0  0.428982  3.191972e-01 -0.954040 -0.410392  0.471574   \n",
      "corr_1_2  4821510.0  0.477967  2.859199e-01 -0.886824 -0.302733  0.519603   \n",
      "r         4821510.0  0.030000  1.457168e-16  0.030000  0.030000  0.030000   \n",
      "T         4821510.0  2.613718  2.229096e+00  0.003968  0.003968  2.099206   \n",
      "S0_0/K    4821510.0  1.883170  9.927582e-01  0.147608  0.540576  1.661343   \n",
      "S0_1/K    4821510.0  1.883084  9.936005e-01  0.135909  0.540361  1.660558   \n",
      "S0_2/K    4821510.0  1.882532  9.921492e-01  0.161586  0.539942  1.660915   \n",
      "price/k   4821510.0  0.228877  2.379248e-01  0.000001  0.000029  0.159373   \n",
      "delta_0   4821510.0  0.144783  2.051010e-01  0.000000  0.000000  0.064698   \n",
      "delta_1   4821510.0  0.143019  2.066010e-01  0.000000  0.000000  0.060371   \n",
      "delta_2   4821510.0  0.142993  2.065300e-01  0.000000  0.000000  0.060410   \n",
      "vega_0    4821510.0 -0.031974  1.915927e-01 -3.043398 -0.636702 -0.006985   \n",
      "vega_1    4821510.0 -0.026813  1.932782e-01 -3.403978 -0.629408 -0.004551   \n",
      "vega_2    4821510.0 -0.026748  1.931379e-01 -5.289182 -0.629348 -0.004577   \n",
      "\n",
      "               99%        max  \n",
      "sigma_0   0.792584   0.800000  \n",
      "sigma_1   0.792613   0.800000  \n",
      "sigma_2   0.792506   0.800000  \n",
      "corr_0_1  0.946428   0.999755  \n",
      "corr_0_2  0.946118   0.999676  \n",
      "corr_1_2  0.937369   0.998835  \n",
      "r         0.030000   0.030000  \n",
      "T         7.337302   7.337302  \n",
      "S0_0/K    5.289995  22.535718  \n",
      "S0_1/K    5.296254  24.631501  \n",
      "S0_2/K    5.282234  20.942490  \n",
      "price/k   1.112162   4.005642  \n",
      "delta_0   0.992758   1.000574  \n",
      "delta_1   0.993347   1.000518  \n",
      "delta_2   0.993240   1.000358  \n",
      "vega_0    0.520611   2.357525  \n",
      "vega_1    0.544180   2.289713  \n",
      "vega_2    0.543069   2.778866  \n",
      "\n",
      "✅ Finished checks.\n"
     ]
    }
   ],
   "source": [
    "# save as: check_train_clean.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PATH = \"Train_Clean.parquet\"\n",
    "\n",
    "# What we expect after cleaning\n",
    "FEATURE_COLS = [\n",
    "    \"sigma_0\",\"sigma_1\",\"sigma_2\",\n",
    "    \"corr_0_1\",\"corr_0_2\",\"corr_1_2\",\n",
    "    \"r\",\"T\",\n",
    "    \"S0_0/K\",\"S0_1/K\",\"S0_2/K\",\n",
    "]\n",
    "TARGET_COLS = [\"price/k\"] + [f\"delta_{i}\" for i in range(3)] + [f\"vega_{i}\" for i in range(3)]\n",
    "EXPECTED = FEATURE_COLS + TARGET_COLS\n",
    "\n",
    "RAW_MUST_NOT_EXIST = [\"K\", \"price\"] + [f\"S0_{i}\" for i in range(3)] + [c for c in [\"price_se\"] if True]\n",
    "\n",
    "def main():\n",
    "    if not Path(PATH).exists():\n",
    "        raise FileNotFoundError(f\"{PATH} not found\")\n",
    "\n",
    "    df = pd.read_parquet(PATH, engine=\"pyarrow\")\n",
    "    print(f\"\\n=== Loaded {PATH} → shape {df.shape} ===\")\n",
    "\n",
    "    # -------- 1) Schema checks --------\n",
    "    missing = [c for c in EXPECTED if c not in df.columns]\n",
    "    extra   = [c for c in df.columns if c not in EXPECTED]\n",
    "    raw_left = [c for c in RAW_MUST_NOT_EXIST if c in df.columns] + \\\n",
    "               [c for c in df.columns if c.startswith(\"S0_\") and \"/K\" not in c]\n",
    "\n",
    "    print(\"\\n[Schema]\")\n",
    "    print(\"Missing expected columns :\", missing or \"None ✅\")\n",
    "    print(\"Unexpected extra columns :\", extra or \"None ✅\")\n",
    "    print(\"RAW columns still present:\", raw_left or \"None ✅\")\n",
    "\n",
    "    # Hard fail if required columns are missing\n",
    "    if missing:\n",
    "        print(\"\\n❌ Schema incomplete — fix before training.\")\n",
    "        return\n",
    "\n",
    "    # -------- 2) NA / Inf --------\n",
    "    print(\"\\n[NA / Inf]\")\n",
    "    na = df[EXPECTED].isna().sum()\n",
    "    na = na[na > 0]\n",
    "    print(\"NA counts:\", (\"None ✅\" if na.empty else f\"\\n{na}\"))\n",
    "    inf_any = np.isinf(df[EXPECTED].select_dtypes(include=[float,int])).any().any()\n",
    "    print(\"Any ±Inf? \", \"No ✅\" if not inf_any else \"Yes ❌\")\n",
    "\n",
    "    # -------- 3) Generator-implied ranges --------\n",
    "    print(\"\\n[Range checks (from your data generator)]\")\n",
    "    def frac_bad(col, lo=None, hi=None):\n",
    "        s = df[col]\n",
    "        bad = np.zeros(len(s), dtype=bool)\n",
    "        if lo is not None: bad |= (s < lo - 1e-6)\n",
    "        if hi is not None: bad |= (s > hi + 1e-6)\n",
    "        return 100 * bad.mean()\n",
    "\n",
    "    print(f\"sigma_0 in [0.05,0.8]: {frac_bad('sigma_0',0.05,0.8):.3f}% {'✅' if frac_bad('sigma_0',0.05,0.8)==0 else '⚠️'}\")\n",
    "    print(f\"sigma_1 in [0.05,0.8]: {frac_bad('sigma_1',0.05,0.8):.3f}% {'✅' if frac_bad('sigma_1',0.05,0.8)==0 else '⚠️'}\")\n",
    "    print(f\"sigma_2 in [0.05,0.8]: {frac_bad('sigma_2',0.05,0.8):.3f}% {'✅' if frac_bad('sigma_2',0.05,0.8)==0 else '⚠️'}\")\n",
    "    print(f\"r == 0.03             : { (df['r']!=0.03).mean()*100:.3f}% {'✅' if (df['r']==0.03).all() else '⚠️'}\")\n",
    "    print(f\"T in [0.00397,7.34]   : {frac_bad('T',0.00397,7.34):.3f}% {'✅' if frac_bad('T',0.00397,7.34)==0 else '⚠️'}\")\n",
    "\n",
    "    for c in [\"corr_0_1\",\"corr_0_2\",\"corr_1_2\"]:\n",
    "        bad = ((df[c] < -1-1e-6) | (df[c] > 1+1e-6)).mean()*100\n",
    "        print(f\"{c:>8s} in [-1,1]       : {bad:.3f}% {'✅' if bad==0 else '⚠️'}\")\n",
    "\n",
    "    for c in [\"S0_0/K\",\"S0_1/K\",\"S0_2/K\"]:\n",
    "        bad = (df[c] <= 0).mean()*100\n",
    "        print(f\"{c:>8s} > 0            : {bad:.3f}% {'✅' if bad==0 else '⚠️'}\")\n",
    "\n",
    "    bad = (df[\"price/k\"] < 1e-6).mean()*100\n",
    "    print(f\"{'price/k':>8s} ≥ 1e-6       : {bad:.3f}% {'✅' if bad==0 else '⚠️'}\")\n",
    "\n",
    "    # -------- 4) Delta sanity (worst-of call) --------\n",
    "    print(\"\\n[Delta sanity]\")\n",
    "    D = df[[f\"delta_{i}\" for i in range(3)]].to_numpy()\n",
    "    in_bounds = ((D >= -1e-6) & (D <= 1+1e-6)).all(axis=1).mean()*100\n",
    "    sum_le_1  = (D.sum(axis=1) <= 1 + 1e-6).mean()*100\n",
    "    print(f\"each delta_i in [0,1] : {in_bounds:.2f}% {'✅' if in_bounds==100 else '⚠️'}\")\n",
    "    print(f\"sum(delta_i) ≤ 1      : {sum_le_1:.2f}% {'✅' if sum_le_1==100 else '⚠️'}\")\n",
    "\n",
    "    # -------- 5) Vega scaling sanity (the key check) --------\n",
    "    print(\"\\n[Vega scaling sanity]\")\n",
    "    V = df[[f\"vega_{i}\" for i in range(3)]].copy()\n",
    "    desc = V.describe(percentiles=[0.01,0.5,0.99]).T[[\"mean\",\"std\",\"min\",\"1%\",\"50%\",\"99%\",\"max\"]]\n",
    "    print(desc)\n",
    "\n",
    "    # Heuristic flag: if many |vega| > 20, likely unscaled by K\n",
    "    frac_large = (V.abs().gt(20).any(axis=1)).mean()*100\n",
    "    print(f\"Rows with any |vega| > 20: {frac_large:.3f}% {'✅ (scaled)' if frac_large==0 else '❌ (likely unscaled)'}\")\n",
    "\n",
    "    # -------- 6) Quick overall stats print --------\n",
    "    print(\"\\n[Quick .describe() of all features/targets]\")\n",
    "    print(df[EXPECTED].describe(percentiles=[0.01,0.5,0.99]).T)\n",
    "\n",
    "    print(\"\\n✅ Finished checks.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
