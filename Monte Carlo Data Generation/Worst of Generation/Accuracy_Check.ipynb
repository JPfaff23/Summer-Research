{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e44ec06",
   "metadata": {},
   "source": [
    "# Accuracy across 20 rows at 10M and 100M paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59c962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRICE diagnostics:\n",
      "\n",
      "mean_10M           20.575977\n",
      "se_mean_10M         0.003528\n",
      "mean_100M          23.098340\n",
      "se_mean_100M        0.001233\n",
      "diff               -2.522363\n",
      "pooled_se           0.003737\n",
      "z                -674.890099\n",
      "significant?            True\n",
      "same_3dp?              False\n",
      "accurate_10M?          False\n",
      "accurate_100M?         False\n",
      "Name: price, dtype: object\n",
      "\n",
      "Full comparison table:\n",
      "\n",
      "         mean_10M  se_mean_10M  mean_100M  se_mean_100M      diff  pooled_se  \\\n",
      "metric                                                                         \n",
      "delta_0  0.219971     0.000170   0.310645      0.000071 -0.090674   0.000184   \n",
      "delta_1  0.149243     0.000325   0.108130      0.000067  0.041113   0.000332   \n",
      "delta_2  0.107227     0.000104   0.096484      0.000034  0.010742   0.000109   \n",
      "gamma_0 -4.162500     0.016171   4.431250      0.006797 -8.593750   0.017541   \n",
      "gamma_2 -6.606250     0.006380  -5.940625      0.002079 -0.665625   0.006710   \n",
      "price   20.575977     0.003528  23.098340      0.001233 -2.522363   0.003737   \n",
      "theta   -0.298389     0.007527  -0.537662      0.002432  0.239273   0.007911   \n",
      "\n",
      "                  z  significant?  same_3dp?  accurate_10M?  accurate_100M?  \n",
      "metric                                                                       \n",
      "delta_0 -492.866736          True      False           True            True  \n",
      "delta_1  123.704875          True      False           True            True  \n",
      "delta_2   98.609272          True      False           True            True  \n",
      "gamma_0 -489.916273          True      False          False           False  \n",
      "gamma_2  -99.198776          True      False          False           False  \n",
      "price   -674.890099          True      False          False           False  \n",
      "theta     30.247100          True      False          False           False  \n",
      "\n",
      "ACCURACY SUMMARY:\n",
      "\n",
      "Metrics matching 3dp       : None\n",
      "Metrics failing  3dp       : delta_0, delta_1, delta_2, gamma_0, gamma_2, price, theta\n",
      "Significantly different    : delta_0, delta_1, delta_2, gamma_0, gamma_2, price, theta\n",
      "Accurate at 3dp (10M run)  : delta_0, delta_1, delta_2\n",
      "Accurate at 3dp (100M run) : delta_0, delta_1, delta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Checks:\n",
    "1. Statistical significance between 10M and 100M runs using MC-SE of the *mean*.\n",
    "2. Whether the two runs match to 3 dp (|Δmean| < tol).\n",
    "3. Whether each run’s own SE is small enough (SE_of_mean < tol).\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "# ── edit these paths if needed ──────────────────────────────────\n",
    "FILE_10M  = \"results_10M.parquet\"\n",
    "FILE_100M = \"results_100M.parquet\"\n",
    "\n",
    "# ── knobs ───────────────────────────────────────────────────────\n",
    "ABS_TOL = 5e-4       # tolerance on mean difference for 3dp\n",
    "Z_CRIT  = 1.96       # two-tailed z-critical for α = 0.05\n",
    "\n",
    "\n",
    "def load_dfs():\n",
    "    df10 = pd.read_parquet(FILE_10M)\n",
    "    df100 = pd.read_parquet(FILE_100M)\n",
    "    return df10, df100\n",
    "\n",
    "\n",
    "def metrics_from_df(df):\n",
    "    \"\"\"\n",
    "    Identify all metrics that have a matching SE column in the dataframe.\n",
    "    \"\"\"\n",
    "    metrics = set()\n",
    "    for col in df.columns:\n",
    "        m = re.match(r\"(.+)_se(?:_(\\d+))?\", col)\n",
    "        if m:\n",
    "            prefix, idx = m.group(1), m.group(2)\n",
    "            metric = f\"{prefix}_{idx}\" if idx else prefix\n",
    "            metrics.add(metric)\n",
    "    return sorted(metrics)\n",
    "\n",
    "\n",
    "def compute_run_se_of_mean(se_series):\n",
    "    \"\"\"\n",
    "    Compute SE of the mean across rows:\n",
    "      SE_mean = sqrt(sum(se_i**2)) / N_rows\n",
    "    \"\"\"\n",
    "    return np.sqrt((se_series.values**2).sum()) / len(se_series)\n",
    "\n",
    "\n",
    "def compare(df_a, df_b, tol=ABS_TOL, zcrit=Z_CRIT):\n",
    "    mets = metrics_from_df(df_a)\n",
    "    rows = []\n",
    "    for m in mets:\n",
    "        # determine se column name\n",
    "        if '_' in m:\n",
    "            base, idx = m.rsplit('_',1)\n",
    "            se_col = f\"{base}_se_{idx}\"\n",
    "        else:\n",
    "            se_col = f\"{m}_se\"\n",
    "\n",
    "        try:\n",
    "            # compute means\n",
    "            ma = df_a[m].mean()\n",
    "            mb = df_b[m].mean()\n",
    "            # compute SE of mean\n",
    "            sea = compute_run_se_of_mean(df_a[se_col])\n",
    "            seb = compute_run_se_of_mean(df_b[se_col])\n",
    "        except Exception:\n",
    "            # missing columns or other error: skip\n",
    "            continue\n",
    "\n",
    "        # skip if any is not finite\n",
    "        if not all(np.isfinite([ma, mb, sea, seb])):\n",
    "            continue\n",
    "\n",
    "        # z-test for mean difference\n",
    "        diff       = ma - mb\n",
    "        pooled_se  = sqrt(sea**2 + seb**2)\n",
    "        zscore     = diff / pooled_se\n",
    "        significant= abs(zscore) > zcrit\n",
    "\n",
    "        # 3dp agreement\n",
    "        same_3dp   = abs(diff) < tol\n",
    "        # intrinsic accuracy\n",
    "        acc_a      = sea < tol\n",
    "        acc_b      = seb < tol\n",
    "\n",
    "        rows.append({\n",
    "            \"metric\":         m,\n",
    "            \"mean_10M\":       ma,\n",
    "            \"se_mean_10M\":    sea,\n",
    "            \"mean_100M\":      mb,\n",
    "            \"se_mean_100M\":   seb,\n",
    "            \"diff\":           diff,\n",
    "            \"pooled_se\":      pooled_se,\n",
    "            \"z\":              zscore,\n",
    "            \"significant?\":   significant,\n",
    "            \"same_3dp?\":      same_3dp,\n",
    "            \"accurate_10M?\":  acc_a,\n",
    "            \"accurate_100M?\": acc_b,\n",
    "        })\n",
    "    return pd.DataFrame(rows).set_index(\"metric\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    df10, df100 = load_dfs()\n",
    "    cmp = compare(df10, df100)\n",
    "\n",
    "    # PRICE diagnostics\n",
    "    print(\"\\nPRICE diagnostics:\\n\")\n",
    "    print(cmp.loc[\"price\", [\n",
    "        \"mean_10M\",\"se_mean_10M\",\n",
    "        \"mean_100M\",\"se_mean_100M\",\n",
    "        \"diff\",\"pooled_se\",\"z\",\n",
    "        \"significant?\",\"same_3dp?\",\n",
    "        \"accurate_10M?\",\"accurate_100M?\"\n",
    "    ]])\n",
    "\n",
    "    # Full comparison table\n",
    "    pd.set_option(\"display.float_format\", \"{:0.6f}\".format)\n",
    "    print(\"\\nFull comparison table:\\n\")\n",
    "    print(cmp)\n",
    "\n",
    "    # Accuracy summary\n",
    "    print(\"\\nACCURACY SUMMARY:\\n\")\n",
    "    match3 = cmp.index[cmp[\"same_3dp?\"]].tolist()\n",
    "    fail3  = cmp.index[~cmp[\"same_3dp?\"]].tolist()\n",
    "    sig    = cmp.index[cmp[\"significant?\"]].tolist()\n",
    "    acc10  = cmp.index[cmp[\"accurate_10M?\"]].tolist()\n",
    "    acc100 = cmp.index[cmp[\"accurate_100M?\"]].tolist()\n",
    "\n",
    "    print(f\"Metrics matching 3dp       : {', '.join(match3) or 'None'}\")\n",
    "    print(f\"Metrics failing  3dp       : {', '.join(fail3) or 'None'}\")\n",
    "    print(f\"Significantly different    : {', '.join(sig) or 'None'}\")\n",
    "    print(f\"Accurate at 3dp (10M run)  : {', '.join(acc10) or 'None'}\")\n",
    "    print(f\"Accurate at 3dp (100M run) : {', '.join(acc100) or 'None'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2518ea",
   "metadata": {},
   "source": [
    "# Ferguson and Green Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae59449",
   "metadata": {},
   "source": [
    "## Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "886117c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price = 54.250000,  SE = 0.002989\n",
      "\n",
      "1 cent (0.01)  : PASS  (SE = 0.002989 < 0.010000)\n",
      "0.1 cent (0.001): FAIL  (SE = 0.002989 > 0.001000)\n",
      "0.01 cent (0.0001): FAIL  (SE = 0.002989 > 0.000100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, torch\n",
    "from make_worst_of import fg_sample, price_mc, SEED_BASE\n",
    "\n",
    "# 1) Fix your seeds for full reproducibility\n",
    "np.random.seed(SEED_BASE)\n",
    "torch.manual_seed(SEED_BASE)\n",
    "\n",
    "# 2) Draw one scenario and compute price + SE\n",
    "params = fg_sample()\n",
    "price, se = price_mc(\n",
    "    params,\n",
    "    n_paths= 100_000_000,\n",
    "    n_steps=64,\n",
    "    return_se=True\n",
    ")\n",
    "\n",
    "# 3) Define your accuracy thresholds (in absolute price‐error units)\n",
    "thresholds = {\n",
    "    \"1 cent (0.01)\":    0.01,\n",
    "    \"0.1 cent (0.001)\": 0.001,\n",
    "    \"0.01 cent (0.0001)\": 0.0001,\n",
    "}\n",
    "\n",
    "# 4) Print results\n",
    "print(f\"price = {price:.6f},  SE = {se:.6f}\\n\")\n",
    "for label, tol in thresholds.items():\n",
    "    status = \"PASS\" if se < tol else \"FAIL\"\n",
    "    print(f\"{label:15}: {status}  (SE = {se:.6f} {'<' if status=='PASS' else '>'} {tol:.6f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb756232",
   "metadata": {},
   "source": [
    "Able to replicate the Ferguson and Green of 1 cent accuracy with both 10 Mil and 100 Mil paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c64ff",
   "metadata": {},
   "source": [
    "## Variance Reduction Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862fdbe9",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Find an effective method to drastically reduce the Monte Carlo standard error (variance) for a robust test dataset of worst‑of option payoffs.\n",
    "\n",
    "---\n",
    "\n",
    "## Methods Attempted\n",
    "\n",
    "1. **Sobol/QMC**\n",
    "   Owen‑scrambled Sobol quasi‑Monte Carlo for low‑discrepancy sampling.\n",
    "\n",
    "2. **Antithetic Variates**\n",
    "   Pairing each Sobol point with its antithetic counterpart.\n",
    "\n",
    "3. **Brownian Bridge**\n",
    "   Reordering the time increments to concentrate variance in early steps.\n",
    "\n",
    "4. **Exponential Tilting (Importance Sampling)**\n",
    "   Tilting the asset Brownian drifts to overweight scenarios where the payoff is nonzero, with likelihood‑ratio correction.\n",
    "\n",
    "5. **Control Variate: Sum of Vanilla Calls**\n",
    "   Adding the sum of individual call payoffs as a crude variate (β=1).\n",
    "\n",
    "6. **Regression‑based Control Variate**\n",
    "   Estimating the optimal β via sample covariance/variance between the target payoff and the call‑sum variate.\n",
    "\n",
    "7. **Geometric‑Basket Control Variate**\n",
    "   Using the analytic geometric‑basket call payoff as a highly correlated variate.\n",
    "\n",
    "8. **Multi‑Level Monte Carlo (MLMC)**\n",
    "   Telescoping coarse and fine time‑step estimates to reduce both bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "* **Base Monte Carlo** with 100 million Sobol+antithetic+bridge paths: SE ≈ 0.00299.\n",
    "* **Regression CV** (sum‑of‑calls): SE ≈ 0.00299 (minimal gain over β=1).\n",
    "* **Geometric‑Basket CV**: moderate improvement but still SE ≳ 0.0025.\n",
    "* **Exponential Tilting + Regression CV**: introduced NaNs at high path counts; once stabilized, SE ≳ 0.0029.\n",
    "\n",
    "*No combination so far has achieved SE ≤ 0.001 on 100 M paths.*\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "* **Fine‑tune Importance Sampling**: search for optimal tilt vector θ to maximize variance reduction.\n",
    "* **Implement MLMC**: start with a two‑level scheme (e.g. 16 vs 64 timesteps) to cut cost and variance.\n",
    "* **Explore Stratification**: stratify by asset index or payoff buckets in conjunction with Sobol.\n",
    "* **Hybrid Methods**: combine tailored tilting, MLMC, and geometric CV for multiplicative gains.\n",
    "\n",
    "*By iterating on these advanced techniques, we aim for another 5×–10× reduction in SE.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e6efcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1000000, 16, 3]' is invalid for input of size 192000000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 164\u001b[0m\n\u001b[0;32m    162\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(SEED_BASE)\n\u001b[0;32m    163\u001b[0m params \u001b[38;5;241m=\u001b[39m fg_sample()\n\u001b[1;32m--> 164\u001b[0m price, se \u001b[38;5;241m=\u001b[39m \u001b[43mprice_mlmc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps_fine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_se\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLMC price = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprice\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, SE = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 135\u001b[0m, in \u001b[0;36mprice_mlmc\u001b[1;34m(params, n_paths, n_steps_fine, return_se)\u001b[0m\n\u001b[0;32m    133\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(CHUNK_PATH, per1\u001b[38;5;241m-\u001b[39moffset)\n\u001b[0;32m    134\u001b[0m pay_f \u001b[38;5;241m=\u001b[39m mc_paths_payoffs(params, c, n_steps_fine, engine1, dev)\n\u001b[1;32m--> 135\u001b[0m pay_c \u001b[38;5;241m=\u001b[39m \u001b[43mmc_paths_payoffs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps_coarse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m diff  \u001b[38;5;241m=\u001b[39m pay_f \u001b[38;5;241m-\u001b[39m pay_c\n\u001b[0;32m    137\u001b[0m discd \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m diff\n",
      "File \u001b[1;32mc:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[1], line 77\u001b[0m, in \u001b[0;36mmc_paths_payoffs\u001b[1;34m(params, m, n_steps, engine, device)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmc_paths_payoffs\u001b[39m(params, m, n_steps, engine, device):\n\u001b[1;32m---> 77\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_qmc_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_ASSETS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     S0    \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS0\u001b[39m\u001b[38;5;124m'\u001b[39m],   device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     79\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m],device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mgenerate_qmc_paths\u001b[1;34m(engine, m, n_steps, d, device)\u001b[0m\n\u001b[0;32m     69\u001b[0m u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([u,\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m-\u001b[39mu],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     70\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m normals \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43micdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m brownian_bridge(normals)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1000000, 16, 3]' is invalid for input of size 192000000"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "make_worst_of_dataset_fast_mlmc_welford.py\n",
    "  • multi-GPU, single-process Monte Carlo in FP32 with two-level MLMC\n",
    "  • Owen-scrambled Sobol, antithetic, Brownian bridge\n",
    "  • Two-level MLMC: coarse (n_steps/4) + fine (n_steps)\n",
    "  • Streaming Welford algorithm for mean/variance (O(1) RAM)\n",
    "  • Chunk-safe up to 100 M paths on 4×12 GiB GPUs\n",
    "  • Exports price & SE (no Parquet write here)\n",
    "\"\"\"\n",
    "\n",
    "import os, math, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Beta, Normal\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "# ─────────────────────────── knobs ────────────────────────────\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark       = True\n",
    "\n",
    "# Problem constants\n",
    "N_ASSETS   = 3\n",
    "R_RATE     = 0.03\n",
    "SEED_BASE  = 42\n",
    "CHUNK_PATH = 1_000_000  # inner GPU chunk size per device\n",
    "\n",
    "# CUDA devices\n",
    "NGPU    = torch.cuda.device_count()\n",
    "DEVICES = [torch.device(f\"cuda:{i}\") for i in range(NGPU)]\n",
    "if NGPU == 0:\n",
    "    sys.exit(\"No CUDA GPU visible – aborting.\")\n",
    "\n",
    "# ───────────────── correlation sampler ───────────────────────\n",
    "def cvine_corr(d, a=5.0, b=2.0):\n",
    "    beta = Beta(torch.tensor([a],device=\"cuda\"), torch.tensor([b],device=\"cuda\"))\n",
    "    P = torch.eye(d,device=\"cuda\")\n",
    "    for k in range(d-1):\n",
    "        for i in range(k+1,d):\n",
    "            rho = 2*beta.sample().item() - 1.0\n",
    "            for m in range(k-1,-1,-1):\n",
    "                rho = rho*math.sqrt((1-P[m,i]**2)*(1-P[m,k]**2)) + P[m,i]*P[m,k]\n",
    "            P[k,i]=P[i,k]=rho\n",
    "    ev,evec = torch.linalg.eigh(P)\n",
    "    return evec @ torch.diag(torch.clamp(ev,min=1e-6)) @ evec.T\n",
    "\n",
    "# ───────────────── sample generator ──────────────────────────\n",
    "def fg_sample():\n",
    "    z = np.random.normal(0.5,0.5,N_ASSETS)\n",
    "    return dict(\n",
    "        S0    = (100*np.exp(z)).astype(np.float32),\n",
    "        sigma = np.random.uniform(0.0,1.0,N_ASSETS).astype(np.float32),\n",
    "        T     = float((np.random.randint(1,44)**2)/252.0),\n",
    "        rho   = cvine_corr(N_ASSETS).cpu().numpy().astype(np.float32),\n",
    "        K     = 100.0,\n",
    "        r     = R_RATE\n",
    "    )\n",
    "\n",
    "# ───────────────── Brownian bridge reorder ───────────────────\n",
    "def brownian_bridge(increments):\n",
    "    order = [increments.shape[1]-1] + list(range(increments.shape[1]-1))\n",
    "    return increments[:,order,:]\n",
    "\n",
    "# ───────────────── QMC + antithetic generator ───────────────\n",
    "def generate_qmc_paths(engine, m, n_steps, d, device):\n",
    "    u = engine.draw(m//2, dtype=torch.float32)\n",
    "    u = torch.cat([u,1.0-u],dim=0).to(device)\n",
    "    u = u.clamp(min=1e-6, max=1-1e-6)\n",
    "    normals = Normal(0.,1.).icdf(u).view(m,n_steps,d)\n",
    "    return brownian_bridge(normals)\n",
    "\n",
    "# ──────────── single-level MC core ──────────────────────────\n",
    "@torch.no_grad()\n",
    "def mc_paths_payoffs(params, m, n_steps, engine, device):\n",
    "    Z = generate_qmc_paths(engine, m, n_steps, N_ASSETS, device)\n",
    "    S0    = torch.tensor(params['S0'],   device=device)\n",
    "    sigma = torch.tensor(params['sigma'],device=device)\n",
    "    T     = torch.tensor(params['T'],    device=device)\n",
    "    rho   = torch.tensor(params['rho'],  device=device)\n",
    "    K, r  = params['K'], params['r']\n",
    "    dt    = T / n_steps\n",
    "    mu    = r - 0.5*sigma**2\n",
    "    sig   = sigma\n",
    "    chol  = torch.linalg.cholesky(rho)\n",
    "\n",
    "    logS = torch.log(S0).expand(m,N_ASSETS).clone()\n",
    "    sqrt_dt = math.sqrt(dt.item())\n",
    "    for k in range(n_steps):\n",
    "        dW   = Z[:,k,:] @ chol.T\n",
    "        logS = logS + mu*dt + sig*sqrt_dt*dW\n",
    "    ST     = torch.exp(logS)\n",
    "    payoff = torch.clamp(ST.min(dim=1).values - K, 0.)\n",
    "    return payoff\n",
    "\n",
    "# ──────────── Two-level MLMC with Welford streaming ─────────────────\n",
    "def price_mlmc(params, n_paths, n_steps_fine, *, return_se=False):\n",
    "    # split paths\n",
    "    n0 = n_paths//2\n",
    "    n1 = n_paths - n0\n",
    "    n_steps_coarse = n_steps_fine//4\n",
    "\n",
    "    # Welford accumulators for coarse payoff\n",
    "    count0, mean0, M2_0 = 0, 0.0, 0.0\n",
    "    # and for fine-coarse difference\n",
    "    count1, mean1, M2_1 = 0, 0.0, 0.0\n",
    "\n",
    "    # Coarse level\n",
    "    for dev_idx, dev in enumerate(DEVICES):\n",
    "        engine0 = SobolEngine(N_ASSETS*n_steps_coarse, scramble=True, seed=SEED_BASE+dev_idx)\n",
    "        per0    = n0 // NGPU\n",
    "        for offset in range(0, per0, CHUNK_PATH):\n",
    "            c = min(CHUNK_PATH, per0-offset)\n",
    "            pay0 = mc_paths_payoffs(params, c, n_steps_coarse, engine0, dev)\n",
    "            disc0 = math.exp(-params['r']*params['T']) * pay0\n",
    "            # chunk stats\n",
    "            k      = disc0.size(0)\n",
    "            x_mean = disc0.mean().item()\n",
    "            x_M2   = ((disc0 - x_mean)**2).sum().item()\n",
    "            # Welford combine\n",
    "            delta  = x_mean - mean0\n",
    "            total  = count0 + k\n",
    "            mean0  = mean0 + delta * k/total\n",
    "            M2_0   = M2_0 + x_M2 + delta**2 * count0 * k/total\n",
    "            count0 = total\n",
    "\n",
    "    # Fine-coarse diff level\n",
    "    for dev_idx, dev in enumerate(DEVICES):\n",
    "        engine1 = SobolEngine(N_ASSETS*n_steps_fine, scramble=True, seed=SEED_BASE+100+dev_idx)\n",
    "        per1    = n1 // NGPU\n",
    "        for offset in range(0, per1, CHUNK_PATH):\n",
    "            c = min(CHUNK_PATH, per1-offset)\n",
    "            pay_f = mc_paths_payoffs(params, c, n_steps_fine, engine1, dev)\n",
    "            pay_c = mc_paths_payoffs(params, c, n_steps_coarse, engine1, dev)\n",
    "            diff  = pay_f - pay_c\n",
    "            discd = math.exp(-params['r']*params['T']) * diff\n",
    "            # chunk stats\n",
    "            k      = discd.size(0)\n",
    "            x_mean = discd.mean().item()\n",
    "            x_M2   = ((discd - x_mean)**2).sum().item()\n",
    "            delta  = x_mean - mean1\n",
    "            total  = count1 + k\n",
    "            mean1  = mean1 + delta * k/total\n",
    "            M2_1   = M2_1 + x_M2 + delta**2 * count1 * k/total\n",
    "            count1 = total\n",
    "\n",
    "    est_price = mean0 + mean1\n",
    "    # compute standard error\n",
    "    var0 = M2_0/(count0-1) if count0>1 else 0.0\n",
    "    var1 = M2_1/(count1-1) if count1>1 else 0.0\n",
    "    se   = math.sqrt(var0/count0 + var1/count1)\n",
    "\n",
    "    if return_se:\n",
    "        return est_price, se\n",
    "    return est_price\n",
    "\n",
    "# ──────────── quick test ───────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np, torch\n",
    "    np.random.seed(SEED_BASE)\n",
    "    torch.manual_seed(SEED_BASE)\n",
    "    params = fg_sample()\n",
    "    price, se = price_mlmc(params, n_paths=10_000_000, n_steps_fine=64, return_se=True)\n",
    "    print(f\"MLMC price = {price:.6f}, SE = {se:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
