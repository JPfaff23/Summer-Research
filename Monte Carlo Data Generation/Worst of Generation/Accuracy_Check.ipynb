{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e44ec06",
   "metadata": {},
   "source": [
    "# Accuracy across 20 rows at 10M and 100M paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59c962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRICE diagnostics:\n",
      "\n",
      "mean_10M           20.575977\n",
      "se_mean_10M         0.003528\n",
      "mean_100M          23.098340\n",
      "se_mean_100M        0.001233\n",
      "diff               -2.522363\n",
      "pooled_se           0.003737\n",
      "z                -674.890099\n",
      "significant?            True\n",
      "same_3dp?              False\n",
      "accurate_10M?          False\n",
      "accurate_100M?         False\n",
      "Name: price, dtype: object\n",
      "\n",
      "Full comparison table:\n",
      "\n",
      "         mean_10M  se_mean_10M  mean_100M  se_mean_100M      diff  pooled_se  \\\n",
      "metric                                                                         \n",
      "delta_0  0.219971     0.000170   0.310645      0.000071 -0.090674   0.000184   \n",
      "delta_1  0.149243     0.000325   0.108130      0.000067  0.041113   0.000332   \n",
      "delta_2  0.107227     0.000104   0.096484      0.000034  0.010742   0.000109   \n",
      "gamma_0 -4.162500     0.016171   4.431250      0.006797 -8.593750   0.017541   \n",
      "gamma_2 -6.606250     0.006380  -5.940625      0.002079 -0.665625   0.006710   \n",
      "price   20.575977     0.003528  23.098340      0.001233 -2.522363   0.003737   \n",
      "theta   -0.298389     0.007527  -0.537662      0.002432  0.239273   0.007911   \n",
      "\n",
      "                  z  significant?  same_3dp?  accurate_10M?  accurate_100M?  \n",
      "metric                                                                       \n",
      "delta_0 -492.866736          True      False           True            True  \n",
      "delta_1  123.704875          True      False           True            True  \n",
      "delta_2   98.609272          True      False           True            True  \n",
      "gamma_0 -489.916273          True      False          False           False  \n",
      "gamma_2  -99.198776          True      False          False           False  \n",
      "price   -674.890099          True      False          False           False  \n",
      "theta     30.247100          True      False          False           False  \n",
      "\n",
      "ACCURACY SUMMARY:\n",
      "\n",
      "Metrics matching 3dp       : None\n",
      "Metrics failing  3dp       : delta_0, delta_1, delta_2, gamma_0, gamma_2, price, theta\n",
      "Significantly different    : delta_0, delta_1, delta_2, gamma_0, gamma_2, price, theta\n",
      "Accurate at 3dp (10M run)  : delta_0, delta_1, delta_2\n",
      "Accurate at 3dp (100M run) : delta_0, delta_1, delta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacks\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Checks:\n",
    "1. Statistical significance between 10M and 100M runs using MC-SE of the *mean*.\n",
    "2. Whether the two runs match to 3 dp (|Δmean| < tol).\n",
    "3. Whether each run’s own SE is small enough (SE_of_mean < tol).\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "# ── edit these paths if needed ──────────────────────────────────\n",
    "FILE_10M  = \"results_10M.parquet\"\n",
    "FILE_100M = \"results_100M.parquet\"\n",
    "\n",
    "# ── knobs ───────────────────────────────────────────────────────\n",
    "ABS_TOL = 5e-4       # tolerance on mean difference for 3dp\n",
    "Z_CRIT  = 1.96       # two-tailed z-critical for α = 0.05\n",
    "\n",
    "\n",
    "def load_dfs():\n",
    "    df10 = pd.read_parquet(FILE_10M)\n",
    "    df100 = pd.read_parquet(FILE_100M)\n",
    "    return df10, df100\n",
    "\n",
    "\n",
    "def metrics_from_df(df):\n",
    "    \"\"\"\n",
    "    Identify all metrics that have a matching SE column in the dataframe.\n",
    "    \"\"\"\n",
    "    metrics = set()\n",
    "    for col in df.columns:\n",
    "        m = re.match(r\"(.+)_se(?:_(\\d+))?\", col)\n",
    "        if m:\n",
    "            prefix, idx = m.group(1), m.group(2)\n",
    "            metric = f\"{prefix}_{idx}\" if idx else prefix\n",
    "            metrics.add(metric)\n",
    "    return sorted(metrics)\n",
    "\n",
    "\n",
    "def compute_run_se_of_mean(se_series):\n",
    "    \"\"\"\n",
    "    Compute SE of the mean across rows:\n",
    "      SE_mean = sqrt(sum(se_i**2)) / N_rows\n",
    "    \"\"\"\n",
    "    return np.sqrt((se_series.values**2).sum()) / len(se_series)\n",
    "\n",
    "\n",
    "def compare(df_a, df_b, tol=ABS_TOL, zcrit=Z_CRIT):\n",
    "    mets = metrics_from_df(df_a)\n",
    "    rows = []\n",
    "    for m in mets:\n",
    "        # determine se column name\n",
    "        if '_' in m:\n",
    "            base, idx = m.rsplit('_',1)\n",
    "            se_col = f\"{base}_se_{idx}\"\n",
    "        else:\n",
    "            se_col = f\"{m}_se\"\n",
    "\n",
    "        try:\n",
    "            # compute means\n",
    "            ma = df_a[m].mean()\n",
    "            mb = df_b[m].mean()\n",
    "            # compute SE of mean\n",
    "            sea = compute_run_se_of_mean(df_a[se_col])\n",
    "            seb = compute_run_se_of_mean(df_b[se_col])\n",
    "        except Exception:\n",
    "            # missing columns or other error: skip\n",
    "            continue\n",
    "\n",
    "        # skip if any is not finite\n",
    "        if not all(np.isfinite([ma, mb, sea, seb])):\n",
    "            continue\n",
    "\n",
    "        # z-test for mean difference\n",
    "        diff       = ma - mb\n",
    "        pooled_se  = sqrt(sea**2 + seb**2)\n",
    "        zscore     = diff / pooled_se\n",
    "        significant= abs(zscore) > zcrit\n",
    "\n",
    "        # 3dp agreement\n",
    "        same_3dp   = abs(diff) < tol\n",
    "        # intrinsic accuracy\n",
    "        acc_a      = sea < tol\n",
    "        acc_b      = seb < tol\n",
    "\n",
    "        rows.append({\n",
    "            \"metric\":         m,\n",
    "            \"mean_10M\":       ma,\n",
    "            \"se_mean_10M\":    sea,\n",
    "            \"mean_100M\":      mb,\n",
    "            \"se_mean_100M\":   seb,\n",
    "            \"diff\":           diff,\n",
    "            \"pooled_se\":      pooled_se,\n",
    "            \"z\":              zscore,\n",
    "            \"significant?\":   significant,\n",
    "            \"same_3dp?\":      same_3dp,\n",
    "            \"accurate_10M?\":  acc_a,\n",
    "            \"accurate_100M?\": acc_b,\n",
    "        })\n",
    "    return pd.DataFrame(rows).set_index(\"metric\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    df10, df100 = load_dfs()\n",
    "    cmp = compare(df10, df100)\n",
    "\n",
    "    # PRICE diagnostics\n",
    "    print(\"\\nPRICE diagnostics:\\n\")\n",
    "    print(cmp.loc[\"price\", [\n",
    "        \"mean_10M\",\"se_mean_10M\",\n",
    "        \"mean_100M\",\"se_mean_100M\",\n",
    "        \"diff\",\"pooled_se\",\"z\",\n",
    "        \"significant?\",\"same_3dp?\",\n",
    "        \"accurate_10M?\",\"accurate_100M?\"\n",
    "    ]])\n",
    "\n",
    "    # Full comparison table\n",
    "    pd.set_option(\"display.float_format\", \"{:0.6f}\".format)\n",
    "    print(\"\\nFull comparison table:\\n\")\n",
    "    print(cmp)\n",
    "\n",
    "    # Accuracy summary\n",
    "    print(\"\\nACCURACY SUMMARY:\\n\")\n",
    "    match3 = cmp.index[cmp[\"same_3dp?\"]].tolist()\n",
    "    fail3  = cmp.index[~cmp[\"same_3dp?\"]].tolist()\n",
    "    sig    = cmp.index[cmp[\"significant?\"]].tolist()\n",
    "    acc10  = cmp.index[cmp[\"accurate_10M?\"]].tolist()\n",
    "    acc100 = cmp.index[cmp[\"accurate_100M?\"]].tolist()\n",
    "\n",
    "    print(f\"Metrics matching 3dp       : {', '.join(match3) or 'None'}\")\n",
    "    print(f\"Metrics failing  3dp       : {', '.join(fail3) or 'None'}\")\n",
    "    print(f\"Significantly different    : {', '.join(sig) or 'None'}\")\n",
    "    print(f\"Accurate at 3dp (10M run)  : {', '.join(acc10) or 'None'}\")\n",
    "    print(f\"Accurate at 3dp (100M run) : {', '.join(acc100) or 'None'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2518ea",
   "metadata": {},
   "source": [
    "# Ferguson and Green Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae59449",
   "metadata": {},
   "source": [
    "## Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "886117c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price = 54.250000,  SE = 0.002989\n",
      "\n",
      "1 cent (0.01)  : PASS  (SE = 0.002989 < 0.010000)\n",
      "0.1 cent (0.001): FAIL  (SE = 0.002989 > 0.001000)\n",
      "0.01 cent (0.0001): FAIL  (SE = 0.002989 > 0.000100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, torch\n",
    "from make_worst_of import fg_sample, price_mc, SEED_BASE\n",
    "\n",
    "# 1) Fix your seeds for full reproducibility\n",
    "np.random.seed(SEED_BASE)\n",
    "torch.manual_seed(SEED_BASE)\n",
    "\n",
    "# 2) Draw one scenario and compute price + SE\n",
    "params = fg_sample()\n",
    "price, se = price_mc(\n",
    "    params,\n",
    "    n_paths= 100_000_000,\n",
    "    n_steps=64,\n",
    "    return_se=True\n",
    ")\n",
    "\n",
    "# 3) Define your accuracy thresholds (in absolute price‐error units)\n",
    "thresholds = {\n",
    "    \"1 cent (0.01)\":    0.01,\n",
    "    \"0.1 cent (0.001)\": 0.001,\n",
    "    \"0.01 cent (0.0001)\": 0.0001,\n",
    "}\n",
    "\n",
    "# 4) Print results\n",
    "print(f\"price = {price:.6f},  SE = {se:.6f}\\n\")\n",
    "for label, tol in thresholds.items():\n",
    "    status = \"PASS\" if se < tol else \"FAIL\"\n",
    "    print(f\"{label:15}: {status}  (SE = {se:.6f} {'<' if status=='PASS' else '>'} {tol:.6f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb756232",
   "metadata": {},
   "source": [
    "Able to replicate the Ferguson and Green of 1 cent accuracy with both 10 Mil and 100 Mil paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c64ff",
   "metadata": {},
   "source": [
    "## Variance Reduction Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862fdbe9",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Find an effective method to drastically reduce the Monte Carlo standard error (variance) for a robust test dataset of worst‑of option payoffs.\n",
    "\n",
    "---\n",
    "\n",
    "## Methods Attempted\n",
    "\n",
    "1. **Sobol/QMC**\n",
    "   Owen‑scrambled Sobol quasi‑Monte Carlo for low‑discrepancy sampling.\n",
    "\n",
    "2. **Antithetic Variates**\n",
    "   Pairing each Sobol point with its antithetic counterpart.\n",
    "\n",
    "3. **Brownian Bridge**\n",
    "   Reordering the time increments to concentrate variance in early steps.\n",
    "\n",
    "4. **Exponential Tilting (Importance Sampling)**\n",
    "   Tilting the asset Brownian drifts to overweight scenarios where the payoff is nonzero, with likelihood‑ratio correction.\n",
    "\n",
    "5. **Control Variate: Sum of Vanilla Calls**\n",
    "   Adding the sum of individual call payoffs as a crude variate (β=1).\n",
    "\n",
    "6. **Regression‑based Control Variate**\n",
    "   Estimating the optimal β via sample covariance/variance between the target payoff and the call‑sum variate.\n",
    "\n",
    "7. **Geometric‑Basket Control Variate**\n",
    "   Using the analytic geometric‑basket call payoff as a highly correlated variate.\n",
    "\n",
    "8. **Multi‑Level Monte Carlo (MLMC)**\n",
    "   Telescoping coarse and fine time‑step estimates to reduce both bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "* **Base Monte Carlo** with 100 million Sobol+antithetic+bridge paths: SE ≈ 0.00299.\n",
    "* **Regression CV** (sum‑of‑calls): SE ≈ 0.00299 (minimal gain over β=1).\n",
    "* **Geometric‑Basket CV**: moderate improvement but still SE ≳ 0.0025.\n",
    "* **Exponential Tilting + Regression CV**: introduced NaNs at high path counts; once stabilized, SE ≳ 0.0029.\n",
    "\n",
    "*No combination so far has achieved SE ≤ 0.001 on 100 M paths.*\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "* **Fine‑tune Importance Sampling**: search for optimal tilt vector θ to maximize variance reduction.\n",
    "* **Implement MLMC**: start with a two‑level scheme (e.g. 16 vs 64 timesteps) to cut cost and variance.\n",
    "* **Explore Stratification**: stratify by asset index or payoff buckets in conjunction with Sobol.\n",
    "* **Hybrid Methods**: combine tailored tilting, MLMC, and geometric CV for multiplicative gains.\n",
    "\n",
    "*By iterating on these advanced techniques, we aim for another 5×–10× reduction in SE.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e6efcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price = 56.346448, SE = 0.002206\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "make_worst_of_dataset_fast_streaming.py\n",
    "  • multi-GPU, single-process Monte-Carlo in FP32\n",
    "  • Owen-scrambled Sobol, antithetic, Brownian bridge\n",
    "  • Streaming Welford algorithm for regression control variate\n",
    "  • Geometric-basket control variate with analytic price\n",
    "  • Chunk-safe up to 100 M paths on 4×12 GiB GPUs\n",
    "  • O(1) RAM / no large tensor concatenations\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Beta, Normal\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark       = True\n",
    "\n",
    "# Constants\n",
    "N_ASSETS   = 3\n",
    "R_RATE     = 0.03\n",
    "SEED_BASE  = 42\n",
    "CHUNK_PATH = 1_000_000    # paths per chunk per GPU\n",
    "\n",
    "# Devices\n",
    "NGPU    = torch.cuda.device_count()\n",
    "DEVICES = [torch.device(f\"cuda:{i}\") for i in range(NGPU)]\n",
    "if NGPU == 0:\n",
    "    sys.exit(\"No CUDA GPU visible – aborting.\")\n",
    "\n",
    "# Correlation sampler\n",
    "\n",
    "def cvine_corr(d, a=5.0, b=2.0):\n",
    "    beta = Beta(torch.tensor([a], device=\"cuda\"), torch.tensor([b], device=\"cuda\"))\n",
    "    P    = torch.eye(d, device=\"cuda\")\n",
    "    for k in range(d-1):\n",
    "        for i in range(k+1, d):\n",
    "            rho = 2*beta.sample().item() - 1.0\n",
    "            for m in range(k-1, -1, -1):\n",
    "                rho = rho*math.sqrt((1-P[m,i]**2)*(1-P[m,k]**2)) + P[m,i]*P[m,k]\n",
    "            P[k,i] = P[i,k] = rho\n",
    "    ev,evec = torch.linalg.eigh(P)\n",
    "    return evec @ torch.diag(torch.clamp(ev,min=1e-6)) @ evec.T\n",
    "\n",
    "# Sample generator\n",
    "\n",
    "def fg_sample():\n",
    "    z = np.random.normal(0.5, 0.5, N_ASSETS)\n",
    "    return dict(\n",
    "        S0    = (100*np.exp(z)).astype(np.float32),\n",
    "        sigma = np.random.uniform(0.0,1.0,N_ASSETS).astype(np.float32),\n",
    "        T     = float((np.random.randint(1,44)**2)/252.0),\n",
    "        rho   = cvine_corr(N_ASSETS).cpu().numpy().astype(np.float32),\n",
    "        K     = 100.0,\n",
    "        r     = R_RATE\n",
    "    )\n",
    "\n",
    "# Brownian bridge\n",
    "\n",
    "def brownian_bridge(Z):\n",
    "    order = [Z.shape[1]-1] + list(range(Z.shape[1]-1))\n",
    "    return Z[:,order,:]\n",
    "\n",
    "# QMC+antithetic path generator\n",
    "\n",
    "def generate_qmc_paths(engine, m, n_steps, d, device):\n",
    "    u = engine.draw(m//2, dtype=torch.float32)\n",
    "    u = torch.cat([u, 1.0-u], dim=0).to(device)\n",
    "    u = u.clamp(min=1e-6, max=1-1e-6)\n",
    "    normals = Normal(0.,1.).icdf(u).view(m, n_steps, d)\n",
    "    return brownian_bridge(normals)\n",
    "\n",
    "# Chunk payoff generator (returns discounted worst-of & geo payoffs)\n",
    "@torch.no_grad()\n",
    "def chunk_payoffs(params, m, n_steps, engine, device):\n",
    "    Z     = generate_qmc_paths(engine, m, n_steps, N_ASSETS, device)\n",
    "    S0    = torch.tensor(params['S0'],   device=device)\n",
    "    sigma = torch.tensor(params['sigma'],device=device)\n",
    "    T     = torch.tensor(params['T'],    device=device)\n",
    "    rho   = torch.tensor(params['rho'],  device=device)\n",
    "    K,r   = params['K'], params['r']\n",
    "\n",
    "    dt    = T / n_steps\n",
    "    mu    = r - 0.5 * sigma**2\n",
    "    sig   = sigma\n",
    "    chol  = torch.linalg.cholesky(rho)\n",
    "\n",
    "    logS = torch.log(S0).expand(m, N_ASSETS).clone()\n",
    "    sqrt_dt = math.sqrt(dt.item())\n",
    "    for k in range(n_steps):\n",
    "        dW   = Z[:,k,:] @ chol.T\n",
    "        logS = logS + mu*dt + sig*sqrt_dt*dW\n",
    "    ST      = torch.exp(logS)\n",
    "    payoff  = torch.clamp(ST.min(dim=1).values - K, 0.)\n",
    "    geo_pay = torch.clamp(torch.exp(logS.mean(dim=1)) - K, 0.)\n",
    "\n",
    "    disc_f = math.exp(-r * T.item())\n",
    "    return disc_f * payoff.cpu(), disc_f * geo_pay.cpu()\n",
    "\n",
    "# Analytic geometric basket call price\n",
    "\n",
    "def geo_call_price(S0, K, r, T, sigma, rho):\n",
    "    # sigma_G^2 = (1/d^2) * sigma^T rho sigma\n",
    "    vec = torch.tensor(sigma, dtype=torch.float64)\n",
    "    R   = torch.tensor(rho,    dtype=torch.float64)\n",
    "    varG = (vec @ (R @ vec)) / (N_ASSETS**2)\n",
    "    sigmaG = math.sqrt(varG.item())\n",
    "    G0 = float(np.prod(S0)**(1/N_ASSETS))\n",
    "    d1 = (math.log(G0/K) + (r + 0.5*sigmaG**2)*T) / (sigmaG*math.sqrt(T))\n",
    "    d2 = d1 - sigmaG*math.sqrt(T)\n",
    "    N  = lambda x: 0.5*(1+math.erf(x/math.sqrt(2)))\n",
    "    return G0*N(d1) - K*math.exp(-r*T)*N(d2)\n",
    "\n",
    "# Streaming Monte Carlo with regression CV\n",
    "\n",
    "def price_mc_stream(params, n_paths, n_steps):\n",
    "    per_gpu = n_paths // NGPU\n",
    "    # running sums\n",
    "    cnt = 0\n",
    "    SP, SP2, SC, SC2, SPC = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    for dev_idx, dev in enumerate(DEVICES):\n",
    "        engine = SobolEngine(N_ASSETS*n_steps, scramble=True, seed=SEED_BASE+dev_idx)\n",
    "        for offset in range(0, per_gpu, CHUNK_PATH):\n",
    "            m = min(CHUNK_PATH, per_gpu - offset)\n",
    "            pay, geo = chunk_payoffs(params, m, n_steps, engine, dev)\n",
    "            # update sums\n",
    "            SP  += pay.sum().item()\n",
    "            SP2 += (pay*pay).sum().item()\n",
    "            SC  += geo.sum().item()\n",
    "            SC2 += (geo*geo).sum().item()\n",
    "            SPC += (pay*geo).sum().item()\n",
    "            cnt += m\n",
    "\n",
    "    # moments\n",
    "    EP  = SP / cnt\n",
    "    EC  = SC / cnt\n",
    "    VarP = SP2/cnt - EP*EP\n",
    "    VarC = SC2/cnt - EC*EC\n",
    "    CovPC= SPC/cnt - EP*EC\n",
    "    beta = CovPC / (VarC + 1e-12)\n",
    "    # corrected price\n",
    "    E_geo = geo_call_price(params['S0'], params['K'], params['r'], params['T'], params['sigma'], params['rho'])\n",
    "    price_cv = EP + beta * (E_geo - EC)\n",
    "    # se\n",
    "    VarPCV = VarP + beta*beta*VarC - 2*beta*CovPC\n",
    "    se = math.sqrt(VarPCV / cnt)\n",
    "    return price_cv, se\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np, torch\n",
    "    np.random.seed(SEED_BASE)\n",
    "    torch.manual_seed(SEED_BASE)\n",
    "    params = fg_sample()\n",
    "    price, se = price_mc_stream(params, n_paths=100_000_000, n_steps=64)\n",
    "    print(f\"price = {price:.6f}, SE = {se:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
