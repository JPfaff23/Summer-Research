{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01229c09",
   "metadata": {},
   "source": [
    "# Monte Carlo & Adjoint-Mode Greeks  \n",
    "**Jackson Pfaff – May 2025**\n",
    "\n",
    "---\n",
    "# 1. Monte Carlo Setup (One-Step GBM)\n",
    "\n",
    "We simulate $N$ correlated assets over the interval $[0,T]$ in a single log-Euler step:\n",
    "\n",
    "1. **Correlation and normal draws**  \n",
    "   Let $\\Sigma \\in \\mathbb{R}^{N \\times N}$ be the assets’ correlation matrix.  \n",
    "   Compute its lower-triangular Cholesky factor $L$ so that $LL^\\top = \\Sigma$.  \n",
    "   Draw:\n",
    "\n",
    "   $$\n",
    "   Z \\sim \\mathcal{N}(\\mathbf{0}, I_N), \\quad Y = LZ \\quad (\\text{then } \\mathrm{Cov}[Y] = \\Sigma)\n",
    "   $$\n",
    "\n",
    "2. **Log-price increment**  \n",
    "   For each asset $i = 1, \\dots, N$, compute:\n",
    "\n",
    "   $$\n",
    "   \\text{drift}_i = \\left( r - \\tfrac{1}{2} \\sigma_i^2 \\right) T, \\quad\n",
    "   \\text{diffusion}_i = \\sigma_i \\sqrt{T} Y_i\n",
    "   $$\n",
    "\n",
    "3. **Terminal price**  \n",
    "   Define the log-argument:\n",
    "\n",
    "   $$\n",
    "   G_i = \\ln S_{0,i} + \\text{drift}_i + \\text{diffusion}_i\n",
    "   $$\n",
    "\n",
    "   Then:\n",
    "\n",
    "   $$\n",
    "   S_i(T) = e^{G_i}\n",
    "   $$\n",
    "\n",
    "4. **Worst-of payoff**  \n",
    "   $$\n",
    "   S^* = \\min_{1 \\le i \\le N} S_i(T), \\quad\n",
    "   A = S^* - K, \\quad\n",
    "   h = \\max(A, 0), \\quad\n",
    "   D = e^{-rT}, \\quad\n",
    "   P = D h\n",
    "   $$\n",
    "\n",
    "5. **Monte Carlo estimator**  \n",
    "   Over $M$ independent paths:\n",
    "\n",
    "   $$\n",
    "   \\widehat{V} = \\frac{1}{M} \\sum_{m=1}^M P^{(m)} =\n",
    "   \\frac{e^{-rT}}{M} \\sum_{m=1}^M \\left( \\min_i S_i^{(m)}(T) - K \\right)^+\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Adjoint-Mode Delta\n",
    "\n",
    "We wish to compute the pathwise sensitivity $\\Delta_i = \\partial P / \\partial S_{0,i}$ via reverse-mode (adjoint) differentiation.\n",
    "\n",
    "1. **Initialize**  \n",
    "   $\\bar{P} = 1$\n",
    "\n",
    "2. **Back through discount**  \n",
    "   $P = D h$ gives:\n",
    "\n",
    "   $$\n",
    "   \\bar{h} = D \\bar{P}, \\quad\n",
    "   \\bar{D} = h \\bar{P}\n",
    "   $$\n",
    "\n",
    "3. **Back through ReLU payoff**  \n",
    "   $h = \\max(A, 0)$ gives:\n",
    "\n",
    "   $$\n",
    "   \\bar{A} = \\mathbb{1}_{\\{A > 0\\}} \\bar{h}\n",
    "   $$\n",
    "\n",
    "4. **Back through subtraction**  \n",
    "   $A = S^* - K$ gives:\n",
    "\n",
    "   $$\n",
    "   \\bar{S}^* = \\bar{A}, \\quad\n",
    "   \\bar{K} = -\\bar{A}\n",
    "   $$\n",
    "\n",
    "5. **Back through minimum**  \n",
    "   $S^* = \\min_i S_i(T)$ gives, for each $i$:\n",
    "\n",
    "   $$\n",
    "   \\bar{S}_i(T) = \\mathbb{1}_{\\{i = i^*\\}} \\bar{S}^*, \\quad\n",
    "   i^* = \\operatorname*{arg\\,min}_i S_i(T)\n",
    "   $$\n",
    "\n",
    "6. **Back through exponential**  \n",
    "   $S_i(T) = e^{G_i}$ gives:\n",
    "\n",
    "   $$\n",
    "   \\bar{G}_i = S_i(T) \\bar{S}_i(T)\n",
    "   $$\n",
    "\n",
    "7. **Back through log-Euler step**  \n",
    "   $G_i = \\ln S_{0,i} + (r - \\tfrac{1}{2} \\sigma_i^2)T + \\sigma_i \\sqrt{T} Y_i$ gives:\n",
    "\n",
    "   $$\n",
    "   \\bar{S}_{0,i} = \\frac{1}{S_{0,i}} \\bar{G}_i\n",
    "   $$\n",
    "\n",
    "Hence the pathwise Delta is:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\Delta_i = \\frac{\\partial P}{\\partial S_{0,i}} =\n",
    "e^{-rT} \\mathbb{1}_{\\{A > 0\\}} \\mathbb{1}_{\\{i = i^*\\}} \\frac{S_i(T)}{S_{0,i}}\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Adjoint-Mode Vega\n",
    "\n",
    "We now compute $\\mathrm{Vega}_i = \\partial P / \\partial \\sigma_i$ by continuing the reverse sweep:\n",
    "\n",
    "8. **Back through log-Euler step ($\\sigma$ branch)**  \n",
    "From the full expression:\n",
    "\n",
    "$$\n",
    "G_i = \\ln S_{0,i} + \\left( r - \\tfrac{1}{2} \\sigma_i^2 \\right) T + \\sigma_i \\sqrt{T} Y_i\n",
    "$$\n",
    "\n",
    "Differentiate with respect to $\\sigma_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial G_i}{\\partial \\sigma_i} = -\\sigma_i T + \\sqrt{T} Y_i\n",
    "$$\n",
    "\n",
    "Then the adjoint is:\n",
    "\n",
    "$$\n",
    "\\bar{\\sigma}_i = \\left( -\\sigma_i T + \\sqrt{T} Y_i \\right) \\bar{G}_i\n",
    "$$\n",
    "\n",
    "Substituting $\\bar{G}_i = S_i(T) \\bar{S}_i(T)$ and the definition of $Y_i = (LZ)_i$:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathrm{Vega}_i = e^{-rT} \\mathbb{1}_{\\{A > 0\\}} \\mathbb{1}_{\\{i = i^*\\}}\n",
    "S_i(T) \\left( -\\sigma_i T + \\sqrt{T} (LZ)_i \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "This completes the in-depth adjoint derivations for both Delta and Vega.\n",
    "\n",
    "---\n",
    "## References\n",
    "\n",
    "- Capriotti, L. (2010) *Fast Greeks by Algorithmic Differentiation*\n",
    "- Ferguson, S. & Green, J. (2018) “Deeply Learning Derivatives,” \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
